{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall_ID</th>\n",
       "      <th>Merged_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1174</td>\n",
       "      <td>405904,414778,594982,806590,336937,619647,1156...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Overall_ID                                         Merged_IDs\n",
       "0        1174  405904,414778,594982,806590,336937,619647,1156..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# J'importe les modules nécessaires pour le script.\n",
    "import requests  # Pour effectuer des requêtes HTTP.\n",
    "from bs4 import BeautifulSoup  # Pour parser le contenu HTML.\n",
    "import pandas as pd  # Pour manipuler et stocker les données sous forme de tableaux.\n",
    "import os  # Pour interagir avec le système de fichiers de l'ordinateur.\n",
    "import time  # Pour introduire des délais dans l'exécution du script.\n",
    "import random  # Pour générer des nombres aléatoires.\n",
    "\n",
    "# Read the CSV file containing species IDs and image IDs\n",
    "csv_file_path = './dataset-mushroom-observer/qualified-dataset/all_ids.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# df = df_full.head(1) # use only first one for testing\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je définis les en-têtes HTTP pour simuler une requête d'un navigateur web moderne.\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Je spécifie le chemin du dossier où les images téléchargées seront enregistrées.\n",
    "img_folder_path = 'D:/LAYER0/MO/MO'\n",
    "\n",
    "# Je vérifie si le dossier spécifié existe et sinon, je le crée.\n",
    "if not os.path.exists(img_folder_path):\n",
    "    os.makedirs(img_folder_path)\n",
    "\n",
    "# J'initialise une session de requêtes HTTP pour maintenir les cookies et autres en-têtes à travers les requêtes.\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)  # J'ajoute les en-têtes définis à la session.\n",
    "\n",
    "# Je fixe l'URL de base du site que je souhaite scraper.\n",
    "base_url = 'https://mushroomobserver.org/'\n",
    "\n",
    "# Je prépare une liste pour stocker les données extraites.\n",
    "data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    species_id = row['Overall_ID']\n",
    "    image_ids = row['Merged_IDs'].split(',')  \n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        page_url = f'{base_url}{image_id}'\n",
    "        url_reference = image_id\n",
    "\n",
    "        sleep_time = random.randint(5, 10)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        try:\n",
    "            response = session.get(page_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            name_tag = soup.find('h1', id='title').find('i')\n",
    "            name = name_tag.get_text(strip=True) if name_tag else f'Unknown_{image_id}'\n",
    "\n",
    "            img_tags = soup.find_all('img', class_='carousel-image')\n",
    "            img_counter = 1\n",
    "            for img_tag in img_tags:\n",
    "                img_url = img_tag.get('data-src')\n",
    "                if img_url:\n",
    "                    img_data = session.get(img_url).content\n",
    "                    img_name = f\"{url_reference}_{name.replace(' ', '_')}_{img_counter}.jpg\"\n",
    "                    img_path = os.path.join(img_folder_path, img_name)\n",
    "                    \n",
    "                    with open(img_path, 'wb') as f:\n",
    "                        f.write(img_data)\n",
    "                    \n",
    "                    data.append({'URL': page_url, 'Name': name, 'Image': img_name})\n",
    "                    img_counter += 1\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during request {page_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "output_path = 'D:/LAYER0/MO/dataset.csv'\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f'Data saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
