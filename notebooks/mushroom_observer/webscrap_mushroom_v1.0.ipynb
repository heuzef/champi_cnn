{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  URL                      Name  \\\n",
      "0      https://mushroomobserver.org/5              Xeromphalina   \n",
      "1      https://mushroomobserver.org/6   Xeromphalina campanella   \n",
      "2      https://mushroomobserver.org/7         Xerocomus zelleri   \n",
      "3      https://mushroomobserver.org/8         Xerocomus zelleri   \n",
      "4      https://mushroomobserver.org/9   Xerocomus subtomentosus   \n",
      "..                                ...                       ...   \n",
      "110  https://mushroomobserver.org/115                   Ramaria   \n",
      "111  https://mushroomobserver.org/116                   Ramaria   \n",
      "112  https://mushroomobserver.org/117  Ramaria rubribrunnescens   \n",
      "113  https://mushroomobserver.org/118         Ramaria largentii   \n",
      "114  https://mushroomobserver.org/119        Ramaria gelatinosa   \n",
      "\n",
      "                                image_1                          image_2  \\\n",
      "0                  5_Xeromphalina_1.jpg                              NaN   \n",
      "1       6_Xeromphalina_campanella_1.jpg                              NaN   \n",
      "2             7_Xerocomus_zelleri_1.jpg                              NaN   \n",
      "3             8_Xerocomus_zelleri_1.jpg                              NaN   \n",
      "4       9_Xerocomus_subtomentosus_1.jpg  9_Xerocomus_subtomentosus_2.jpg   \n",
      "..                                  ...                              ...   \n",
      "110                   115_Ramaria_1.jpg                115_Ramaria_2.jpg   \n",
      "111                   116_Ramaria_1.jpg                              NaN   \n",
      "112  117_Ramaria_rubribrunnescens_1.jpg                              NaN   \n",
      "113         118_Ramaria_largentii_1.jpg                              NaN   \n",
      "114        119_Ramaria_gelatinosa_1.jpg                              NaN   \n",
      "\n",
      "    image_3 image_4  \n",
      "0       NaN     NaN  \n",
      "1       NaN     NaN  \n",
      "2       NaN     NaN  \n",
      "3       NaN     NaN  \n",
      "4       NaN     NaN  \n",
      "..      ...     ...  \n",
      "110     NaN     NaN  \n",
      "111     NaN     NaN  \n",
      "112     NaN     NaN  \n",
      "113     NaN     NaN  \n",
      "114     NaN     NaN  \n",
      "\n",
      "[115 rows x 6 columns]\n",
      "Données sauvegardées à D:/Images_champignons/mushroom_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# J'importe les bibliothèques nécessaires : requests pour réaliser des requêtes HTTP,\n",
    "# BeautifulSoup pour analyser le HTML, pandas pour manipuler les données, et os pour gérer les opérations liées au système de fichiers.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Je configure les en-têtes HTTP pour simuler une requête provenant d'un navigateur web moderne.\n",
    "# Ceci est utile pour éviter d'être bloqué par les mécanismes de protection anti-scraping des sites web.\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Je spécifie le chemin du dossier local où je souhaite enregistrer les images téléchargées.\n",
    "img_folder_path = 'D:/Images_champignons'\n",
    "# Si le dossier spécifié n'existe pas, je le crée pour éviter des erreurs lors de la sauvegarde des fichiers.\n",
    "if not os.path.exists(img_folder_path):\n",
    "    os.makedirs(img_folder_path)\n",
    "\n",
    "# J'initialise une session de requêtes HTTP, ce qui me permet de conserver des cookies entre les requêtes\n",
    "# et d'améliorer la gestion des connexions HTTP.\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)  # J'ajoute l'en-tête User-Agent à la session pour toutes les requêtes.\n",
    "\n",
    "# Je définis l'URL de base du site web que je veux scraper.\n",
    "base_url = 'https://mushroomobserver.org/'\n",
    "\n",
    "# J'initialise une liste vide pour collecter les données que je vais extraire.\n",
    "data = []\n",
    "\n",
    "# Je démarre une boucle pour parcourir les pages du site. Ici, je limite l'exemple aux trois premières pages.\n",
    "for i in range(5, 120):\n",
    "    # Je construis l'URL de chaque page en ajoutant le numéro de page à l'URL de base.\n",
    "    page_url = f'{base_url}{i}'\n",
    "    # Je récupère un identifiant unique de la page à partir de son numéro.\n",
    "    url_reference = str(i)\n",
    "    # J'initialise une liste pour stocker les noms des images trouvées sur chaque page.\n",
    "    img_names = []\n",
    "    \n",
    "    # J'essaie d'effectuer une requête GET pour récupérer le contenu de la page.\n",
    "    try:\n",
    "        response = session.get(page_url)\n",
    "        response.raise_for_status()  # Je vérifie que la requête a réussi sans retourner d'erreur HTTP.\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"Erreur HTTP : {err}\")  # En cas d'erreur, je l'affiche et passe à la page suivante.\n",
    "        continue\n",
    "    \n",
    "    # J'utilise BeautifulSoup pour analyser le contenu HTML de la page.\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Je cherche dans le document HTML la balise <h1> avec l'ID 'title' pour extraire le nom du champignon.\n",
    "    name_tag = soup.find('h1', id='title').find('i')\n",
    "    # Si le nom est trouvé, je l'extrait, sinon j'utilise un nom par défaut.\n",
    "    name = name_tag.get_text(strip=True) if name_tag else f'Unknown_{i}'\n",
    "    \n",
    "    # Je recherche toutes les balises <img> qui contiennent les images des champignons.\n",
    "    img_tags = soup.find_all('img', class_='carousel-image')\n",
    "    img_counter = 1\n",
    "    for img_tag in img_tags:\n",
    "        # Pour chaque image, j'extrait son URL.\n",
    "        img_url = img_tag.get('data-src')\n",
    "        if not img_url:\n",
    "            continue  # Si aucune URL n'est trouvée, je passe à l'image suivante.\n",
    "        \n",
    "        try:\n",
    "            # Je télécharge l'image en utilisant l'URL trouvée.\n",
    "            img_data = session.get(img_url).content\n",
    "            # Je génère un nom de fichier unique pour l'image.\n",
    "            img_name = f\"{url_reference}_{name.replace(' ', '_')}_{img_counter}.jpg\"\n",
    "            img_path = os.path.join(img_folder_path, img_name)  # Je construis le chemin complet du fichier image.\n",
    "            \n",
    "            # J'ouvre le fichier en mode écriture binaire et j'enregistre les données de l'image.\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "            \n",
    "            # J'ajoute le nom du fichier à la liste des noms d'images.\n",
    "            img_names.append(img_name)\n",
    "            img_counter += 1  # J'incrémente le compteur d'images.\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la sauvegarde de l'image : {e}\")  # J'affiche les erreurs rencontrées lors de la sauvegarde des images.\n",
    "    \n",
    "    # Je crée un dictionnaire pour cette page contenant l'URL, le nom du champignon, et les noms des images.\n",
    "    page_data = {'URL': page_url, 'Name': name}\n",
    "    for j, img_name in enumerate(img_names, start=1):\n",
    "        page_data[f'image_{j}'] = img_name\n",
    "    \n",
    "    # J'ajoute les données de la page à ma liste de données.\n",
    "    data.append(page_data)\n",
    "\n",
    "# Je convertis la liste de données en DataFrame pour une manipulation et une analyse plus facile.\n",
    "df = pd.DataFrame(data)\n",
    "# Je redéfinis les colonnes du DataFrame pour inclure toutes les images collectées.\n",
    "max_images = max(len(d) for d in data) - 2\n",
    "df = df.reindex(columns=['URL', 'Name'] + [f'image_{j}' for j in range(1, max_images + 1)])\n",
    "\n",
    "# J'affiche le DataFrame pour vérifier les données extraites.\n",
    "print(df)\n",
    "\n",
    "# Je sauvegarde le DataFrame au format Excel pour une utilisation ultérieure.\n",
    "output_path = 'D:/Images_champignons/mushroom_data.xlsx'\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# Je confirme que les données ont été sauvegardées avec succès.\n",
    "print(f'Données sauvegardées à {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
