# üçÑ Projet de groupe 2024 sur la reconnaissance de champignons üçÑ

> Auteurs : *[Heuzef](https://heuzef.com), Yvan Rolland, Viktoriia Saveleva, Florent Constant*

---

# Rendu N¬∞1 : Rapport d‚ÄôEDA

> Date : *06/2024*

# 1. Introduction

La compr√©hension des champignons est cruciale pour la pr√©servation de la biodiversit√©, la sant√© humaine et l'agriculture durable.

Les champignons ne sont pas des plantes, bien que leur apparente immobilit√© puisse le faire penser. Une distinction simple est que les champignons ne font pas de photosynth√®se, contrairement √† une majorit√© de plantes. En fait, dans l'arbre de la vie, les champignons sont plus proches des animaux que des plantes bien que leur mode de nutrition, paroi cellulaire, reproduction, les distingues √©galement nettement des animaux.

L'arbre de la vie, qui repr√©sente la parent√© entre les organismes vivants, peut √™tre d√©coup√© en six r√®gnes. Les champignons repr√©sentent rien qu'√† eux le r√®gne fongique, qui rassemblerait hypoth√©tiquement jusqu'√† 5 millions d'esp√®ces de champignons. Parmi toutes ces esp√®ces, environ seulement 120 000 ont √©t√© nomm√©es et ‚Äúaccept√©es‚Äù par la communaut√© scientifique [en 2017](https://onlinelibrary.wiley.com/doi/abs/10.1128/9781555819583.ch4).

La reconnaissance de champignons repr√©sente un d√©fi dans le domaine de la vision par ordinateur. En effet, les crit√®res biologiques et le peu d'esp√®ce r√©f√©renc√©s limite √† une reconnaissance peu fiable et sur un √©chantillon insignifiant si l'on souhaite √©tudier l'ensemble du r√®gne fongique.

La classification classique des vivants est sch√©matis√©e ainsi :

| Classification classique des r√®gnes |
|:-----------------------------------:|
| Animal                              |
| Plante                              |
| **Champignon (Fungi)**              |
| Protiste                            |
| Bact√©rie                            |
| Arch√©e                              |

Et les principaux rangs taxonomiques ainsi (ici, un exemple de classification du tr√®s connu "*Champignon de Paris*") :

![taxon.png](img/taxon.png)

On y voit que les champignons sont class√©s (du plus g√©n√©ral au plus sp√©cifique) en divisions, ordres, familles, genres et esp√®ces.

L'√©tat de l'art nous apprend que la reconnaissance des champignons au sens large ne sera possible que sur un √©chantillon tr√®s faible du r√®gne fongique, mais ce n'est pas tout, en effet, la vision par ordinateur effectue un balayage des images matricielle pour comparer les similitudes pour chaque pixel avec des images d√©j√† labellis√©, ainsi, nous d√©pendons de la qualit√© des sources de donn√©s, qui ne repr√©sentent qu'un √©chantillon des ~120 000 esp√®ces scientifiquement nomm√©es sur un total pouvant aller jusqu'√† 5 millions d'esp√®ces th√©orique.

Il existe √©galement la distinction entre macro-champignons et micro-champignons, qui se base sur une combinaison de caract√©ristiques morphologiques, cellulaires, reproductives, √©cologiques et √©conomiques. L'identification pr√©cise des champignons exige des connaissances approfondies en mycologie. Par ailleurs les diff√©rentes sources alertent quand √† la difficult√© de l'identification d'une esp√®ce se basant uniquement sur l'aspect visuel.

√Ä ce jours, il existe approximativement 35000 genres et de champignon sur terre et certain peuvent compter jusqu'√† des milliers esp√®ces nomm√©s, tandis que d'autre peuvent n'en compter qu'une seul.

Une analyse visuelle des diff√©rents rangs taxonomiques sur des √©chantillons de photos extraite de Mushroom Observer nous laisse penser que c'est au niveau de l'esp√®ce que nous pouvons observer les plus de traits caract√©ristiques en vue de r√©aliser une identification visuelle :

### Visuel par ordre :

Des champignons appartenant √† l'ordre des PEZIZALES :

![visu_diff_01.png](img/visu_diff_01.png)

### Visuel par famille :

Des champignons appartenant √† la famille des RUSSULACEAE : 

![visu_diff_02.png](img/visu_diff_02.png)

### Visuel par genre :

Des champignons du genre CANTHARELLUS :

![visu_diff_03.png](img/visu_diff_03.png)

### Visuel par esp√®ce :

Des champignons appartenant √† l'esp√®ce HYPHOLOMA LATERITIUM :

![visu_diff_04.png](img/visu_diff_04.png)

C'est √©galement commun√©ment le niveau d'identification recherch√© car c'est au niveau de l'esp√®ce que sont d√©finies les principales propri√©t√©s d'un champignon, telles que la comestibilit√©.

Nous constatons √©galement que les champignons peuvent avoir des formes si vari√©es que deux champignons de la m√™me esp√®ce peuvent avoir un aspect tr√®s diff√©rent (notamment en fonction de l'age), alors que deux champignons d'esp√®ces diff√©rentes peuvent afficher une tr√®s forte ressemblance.

### Vari√©t√©s de formes dans une m√™me esp√®ce

Pour illustration, deux champignons de l'esp√®ce Coprinus comatus mais visuellement tr√®s diff√©rents :

![visu_diff_05.png](img/visu_diff_05.png)

### Confusions possibles

De m√™me deux champignons de genres diff√©rents visuellement difficiles √† distinguer, ici Clytocibe n√©buleux et Entolome livide :

![visu_diff_06.png](img/visu_diff_06.jpg)

![visu_diff_07.png](img/visu_diff_07.jpg)

# 2. Objectif

Ce premier niveau de connaissance de la probl√©matique d'identification visuelle d'un champignon nous permet de distinguer trois difficult√©s majeures du domaine : 

1. L'immense quantit√© d'esp√®ces existantes, la proximit√© visuelle importante existant entre certaines esp√®ces et la diff√©rence morphologique pouvant exister au sein d'une m√™me esp√®ce.

2. La quantit√© et la qualit√© des donn√©es disponibles seront d√©terminantes pour obtenir un mod√®le performant.

3. Selon nos propres capacit√©s et le temps disponible pour la r√©alisation du projet, nous pourrons fixer diff√©rents niveaux d'objectifs √† atteindre pour notre projet, l'essentiel restant l'aspect p√©dagogique et l'acquisition de comp√©tences.

L'objectif primaire est d'entra√Æner un mod√®le pour la reconnaissance des champignons. Pour atteindre cet objectif, il faudra suivre les √©tapes suivantes :

1. Analyser la taxonomie et d√©finir le niveau sur lequel nous concentrer

2. Analyser les donn√©es disponibles

3. Trier et filtrer les donn√©es

4. Data augmentation (cr√©er de nouveaux √©chantillons d'entra√Ænement en appliquant diverses transformations aux images existantes)

5. Pr√©traitement des donn√©es

6. Poursuivre avec des techniques de deep learning

Nous pourrons donc travailler √† entra√Æner un mod√®le capable d'identifier un nombre plus ou moins grand d'esp√®ces avec le plus de pr√©cision possible. Le niveau de difficult√© pourra donc √™tre modul√© selon le nombre d'esp√®ces introduites mais aussi la ressemblance visuelle entre les diff√©rentes esp√®ces introduites.

Nous pourrons √©galement envisager diff√©rentes approches, par exemple entra√Æner et utiliser un mod√®le pour faire du "boxing", g√©n√©rer des donn√©es artificielles par des transformations des images de notre jeu de donn√©es, essayer de quantifier le volume d'images n√©cessaire pour l'obtention d'un certain niveau de performances ...

# 3. Sources de donn√©es identifi√©es

Les ensembles de donn√©es contenant des champignons sont largement utilis√©s pour l'entra√Ænement des algorithmes de machine learning et de deep learning. Divers ensembles de donn√©es sont disponibles en acc√®s libre pour diff√©rentes finalit√©s.

**UC Irvine Mushroom Database** (https://archive.ics.uci.edu/dataset/73/mushroom) comprend 8 124 enregistrements de donn√©es et 22 attributs. Chaque esp√®ce de champignon est identifi√©e comme une classe de champignons comestibles ou toxiques. Ces donn√©es sont r√©parties en 4 208 champignons comestibles et 3 916 champignons toxiques. De nombreuses approches diff√©rentes sont pr√©sent√©es dans la litt√©rature pour travailler avec ce type de caract√©risation des champignons bas√©e sur les caract√©ristiques physiques (pas d'images). Cependant, dans ce travail, nous nous concentrons principalement sur la reconnaissance d'images, notre attention se portant sur les ensembles de donn√©es d'images.

### Mushroom Observer

[Mushroom Observer](https://mushroomobserver.org) est un site web o√π les gens peuvent t√©l√©charger des observations de champignons. Ces observations incluent diff√©rentes informations sur l'esp√®ce observ√©e, comme le nom, l'emplacement, et la certitude concernant l'esp√®ce sur les images soumises. Le site est bas√© sur des photos prises par un grand nombre d'individus qui ne sont pas n√©cessairement des experts dans ce domaine. La certitude des √©tiquettes de classe, donn√©e par la communaut√© est sur une √©chelle continue de 1 √† 3 (o√π 3 repr√©sente le plus haut niveau de certitude).

L'analyse des images de Mushroom Observer montre deux probl√®mes principaux li√©s √† :

1) **la qualit√© des images**. Il y a beaucoup d'images qui ne sont pas exploitables : sch√©mas, clich√©s microscopiques, etc ...

![mo_useless_pictures.png](img/mo_useless_pictures.png)

*Exemples de photos inexploitables*

2) **le niveau de fiabilit√© de l'attribution de classe.** Le syst√®me de vote pour la classification des champignons ajoute de l'incertitude dans l'attribution de classe.

![mo_confidence.png](img/mo_confidence.png)

Ainsi, la base de donn√©es ne peut pas √™tre utilis√©e telle quelle √† partir du site web et doit √™tre filtr√©e.

### MO106 Database

En analysant la litt√©rature utilisant l'ensemble de donn√©es Mushroom Observer, nous avons trouv√© une base de donn√©es MO106 [disponible en acc√®s libre](https://keplab.mik.uni-pannon.hu/en/mo106eng) o√π les auteurs ont s√©lectionn√© 106 classes de champignons de Mushroom Observer en utilisant les crit√®res suivants : esp√®ces ayant au moins 400 images, images avec certitude ‚â• 2. De plus, pour filtrer automatiquement les images afin d'obtenir une image correcte de champignon (sans objets suppl√©mentaires ou sans champignons), les auteurs ont [form√© un mod√®le CNN sp√©cifique](https://ieeexplore.ieee.org/document/9552053).

Cela a abouti √† un ensemble de donn√©es MO106 contenant 29 100 images r√©parties en 106 classes. La plus grande classe compte 581 √©l√©ments, la plus petite 105, avec une moyenne de 275. Les images, disponibles gratuitement pour le t√©l√©chargement, ont des tailles variant entre 97 √ó 130 (plus petite surface) et 640 √ó 640 (plus grande surface).

Pour une observation nous obtenons :

* ¬† Photos
* ¬† Genre et esp√®ces

### Mushrooms classification - Common genus's images

[Dataset de champignons bas√©s sur des images](https://www.kaggle.com/datasets/maysee/mushrooms-classification-common-genuss-images/data).

Cet ensemble de donn√©es contient 9 dossiers d'images des genres de champignons les plus communs du nord de l'Europe (Agaricus, Amanita, Boletus, Cortinarius, Entoloma, Hygrocybe, Lactarius, Russula et Suillus). Chaque dossier contient entre 300 et 1 500 images s√©lectionn√©es de genres de champignons. Les √©tiquettes correspondent aux noms des dossiers. Des codes de classification utilisant cet ensemble de donn√©es sont √©galement disponibles.

L'avantage de cette base de donn√©es par rapport √† Mushroom Observer est que la classification a √©t√© v√©rifi√©e par la soci√©t√© de mycologie d'Europe du Nord, qui a fourni les sources des champignons les plus communs de cette r√©gion et a v√©rifi√© les donn√©es et les √©tiquettes.

Pour une observation nous obtenons :

* ¬† Photos
* ¬† Genre et esp√®ces

### MycoDB

Le site [mycodb.fr](https://www.mycodb.fr) nous permet d'acqu√©rir des caract√©ristique pr√©cises d'un champignon identifi√© via un nom binominal, pour une observation nous obtenons :

* ¬† Photos
* ¬† Division - Classe - Ordre - Famille
* ¬† Synonymes
* ¬† Chapeau
* ¬† Lames
* ¬† Stipe
* ¬† Saveur
* ¬† Odeur
* ¬† Couleur de la spor√©e
* ¬† Ecologie
* ¬† Comestibilit√©
* ¬† R√©f√©rences bibliographiques

### Wikipedia

[Wikip√©dia](https://fr.wikipedia.org) reste une source d'information tr√®s compl√©mentaire et souvent exhaustive pour en apprendre plus sur un genre ou une esp√®ce de champignon.

## Conclusion

Apr√®s identification de ces diff√©rentes sources de donn√©es nous concluons que Mushroom Observer sera celle qui sera la plus exploitable pour obtenir des donn√©es de qualit√©. Le site dispose d'une API permettant un acc√®s √† la quasi totalit√© des donn√©es, permettant d'obtenir une visualisation pr√©cise du nombre d'esp√®ces r√©pertori√©es ainsi que du nombre d'observations et d'images associ√©es √† chaque esp√®ce.

Par ailleurs le jeu de donn√©es MO106 d√©j√† extraites de Mushroom observer pourrait √™tre une source inint√©ressante car d√©j√† pr√™te √† l'emploi bien que la qualit√© des images s√©lectionn√©e √©chappe √† notre contr√¥le. Cela pourra par exemple donner lieu √† un comparatif de pr√©cision des r√©sultats en fonction de la qualit√© des images en entr√©e.

# 4. Exploration approfondie des donn√©es disponibles sur Mushroom observer

Le principal avantage de Mushroom observer est qu'il met √† disposition une API permettant d'acc√©der a des donn√©es structur√©es issues de sa base. Ces donn√©es nous permettrons de faire une analyse qualitative et quantitative des images disponibles. Les donn√©es ont √©t√© t√©l√©charg√©es au format CSV et sont pr√©sentes sur le d√©p√¥t du projet.

## Principales tables

### table names

Cette table contient l'arborescence des nommages disponibles sur le site, r√©partis en niveaux (rangs) de la mani√®re suivante :

1. forme
2. vari√©t√©
3. sous-esp√®ce
4. esp√®ce
5. stirpes
6. sous-section
7. section
8. sous-genre
9. genre
10. famille
11. ordre
12. classe
13. phylum
14. regne
15. domaine
16. groupe

Nous observons par exemple que le site r√©pertorie √† ce jour 56161 esp√®ces.

### table observations

Cette table permet de quantifier le nombre d'observations r√©alis√©es pour chaque esp√®ce mais aussi de qualifier la fiabilit√© de ces observations : le site offrant un syst√®me participatif, l'identification des champignons est soumise au vote des utilisateurs du site. La note de confiance concernant l'identification d'une observation varie de -3 √† 3. Apr√®s √©valuation du nombre d'observation disponible nous choisirons de ne conserver que celles dont le score de confiance est >= 2.

Le graphique montre que le jeu de donn√©e comprends environs 150k observations rattach√©es √† une esp√®ce avec un niveau de confiance >= 2.

![MO Rangs](img/mo_obs_rang.png)

### table images_observation

Cette table liste les images rattach√©es √† chaque observation. Sans surprise les quantit√©s d'images rattach√©es √† chaque rang sont proportionnelles a la quantit√© d'observations. Nous constatons que pour notre s√©lection de crit√®res environs 500k images sont disponibles.

![mo_img_rang.png](img/mo_img_rang.png)

## Agr√©gation par esp√®ces

Nous savons que nous devons disposer d'une quantit√© minimale d'images pour chacune des esp√®ces sur lesquelles nous souhaitons entra√Æner notre mod√®le. Bien que cette quantit√© soit encore √† d√©finir pr√©cis√©ment, nous estimons que 150-250 images serait une base de d√©part viable. Nous constatons aussi que la moiti√© environs des images est exploitable, le reste n'√©tant pas directement des photographie des sp√©cimens de champignons. 

Un second filtrage est effectu√© pour ne s√©lectionner que les esp√®ces qui disposent d'au moins 500 photos sur le site. Nous pouvons donc compter disposer de donn√©es suffisantes pour 129 esp√®ces.

![mo_img_129_species.png](img/mo_img_129_species.png)

## S√©lection finale des images

Nous avons identifi√© le besoin de filtrer manuellement les images avant pr√©-traitement pour exclure celle qui ne sont pas exploitables (sch√©mas, clich√©s microscopiques, etc ...). Nous avons donc r√©alis√© un outil proposant une interface permettant de r√©aliser le tri de mani√®re relativement efficace. Nous pourrons constituer un jeu de donn√©es d'images tri√©e plus ou moins important selon les besoins et le temps disponible au fil de l'avanc√©e du projet.

L'outil est disponible sur le d√©p√¥t du projet.

![mo_manual_select_tool.png](img/mo_manual_select_tool.png)

Une fois la s√©lection effectu√©e, nous pouvons alors ex√©cuter le script de webscraping nous permettant de t√©l√©charger les photos s√©lectionn√©s *(cf: Annexes)*. Pour certains champignons, nous avons plus d'une photo. Nous nous concentrons uniquement sur la premi√®re (le script s√©lectionne uniquement la premi√®re image de la s√©rie).

A la date de r√©daction de ce rapport nous avons r√©uni 2282 images appartenant √† 14 esp√®ces diff√©rentes.

## Organisation des donn√©es

Le stockage des donn√©es, (dans espace de stockage priv√©), est structur√©e ainsi :

```
data
‚îú‚îÄ‚îÄ LAYER0
‚îÇ   ‚îú‚îÄ‚îÄ MO
‚îÇ       ‚îú‚îÄ‚îÄ MO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ MO_106
‚îÇ       ‚îú‚îÄ‚îÄ MO_106
‚îÇ       ‚îú‚îÄ‚îÄ class_stats.csv
‚îÇ       ‚îú‚îÄ‚îÄ dispersion.csv
‚îÇ       ‚îî‚îÄ‚îÄ image_stats.csv
‚îú‚îÄ‚îÄ LAYER1
‚îÇ   ‚îî‚îÄ‚îÄ MO
‚îÇ       ‚îî‚îÄ‚îÄ MO
‚îî‚îÄ‚îÄ LAYER2
    ‚îî‚îÄ‚îÄ MO
        ‚îî‚îÄ‚îÄ MO
```

> Cette configuration nous permettra ult√©rieurement de fournir la base d'image MO ou MO_106 √† nos diff√©rent mod√®les facilement.

# 5. Pr√©-traitement des donn√©es

## Choix des outils de preprocessing

Pour le pr√©traitement des donn√©es, nous avons s√©lectionn√© des outils sp√©cifiques de preprocessing. Ensuite, nous appliquons un second traitement avec le mod√®le YOLOv5 (You Only Look Once version 5), qui permet une d√©tection rapide et pr√©cise des champignons en les identifiant par encadrement *(cf: Annexes)*.

Cela nous permet d'obtenir des images pr√©cise indispensables pour les √©tapes suivantes d'entra√Ænement de mod√®le. Cet outil n'√©tant pas parfait, nous compensons les √©checs de celui-ci avec un outil d'encadrement manuel d√©velopp√© pour l'occasion *(cf: Annexes)*.

Afin de pr√©parer nos images pour les entra√Ænements √† venir, nous appliquons les m√©thodes conventionnelles et r√©currentes pour le CNN.

## Redimensionnement des images

La r√©duction des images √† une taille de 224 x 224 pixels est couramment utilis√©e dans les architectures de r√©seaux de neurones convolutionnels (CNN) pour plusieurs raisons pratiques et techniques tel que la standardisation, la gestion de la m√©moire et des ressources computationnelles, la comparaison avec les mod√®les pr√©-entra√Æn√©s et la capture des caract√©ristiques importantes.

## Enrichir le jeu de donn√©es

Nous r√©alisons une √©tape de r√©-√©chantillonnage afin d'augmenter le volume de donn√©es d'entra√Ænement, pour les futurs mod√®les que nous testerons. Cela nous permettra d'am√©liorer la pr√©cision des mod√®les.

Nous avons r√©alis√© un script exploitant ImageDataGenerator de la librairie tensorflow.keras.preprocessing.image *(cf: Annexes)*.

Nous effectuons ainsi l'augmentation des donn√©es avec les techniques suivantes :

- Rotations

- Retournement horizontal

- Retournement vertical

- Cisaillements

Cela permet de cr√©er de nouveaux √©chantillons d'entra√Ænement √† partir des images existantes, augmentant ainsi la robustesse et la capacit√© de g√©n√©ralisation de notre mod√®le.

![images_augmentees.png](img/images_augmentees.png)
*Exemple d'images enrichies g√©n√©r√©e depuis une unique image source, √† l'aide de la classe ImageDataGenerator de Keras.*

# 6. Conclusion

Ce rapport nous permet d'avoir un aper√ßu de la complexit√© de la reconnaissance de champignons, mettant en lumi√®re les d√©fis et les opportunit√©s qui se pr√©sentent dans ce domaine. √Ä travers une exploration d√©taill√©e de la taxonomie, des sources de donn√©es disponibles et des √©tapes de pr√©traitement des donn√©es, nous avons identifi√© les diff√©rentes options √† explorer pour atteindre notre objectif de d√©veloppement d'un mod√®le de reconnaissance de champignons fiable.

L'analyse a r√©v√©l√© plusieurs d√©fis majeurs, notamment la grande diversit√© des esp√®ces de champignons, la variabilit√© morphologique au sein d'une m√™me esp√®ce, et la qualit√© variable des donn√©es disponibles, n√©cessitant ainsi des strat√©gies de filtration et de pr√©traitement rigoureuses. Cependant, nous avons √©galement identifi√© des sources de donn√©es prometteuses, qui offrent des ensembles de donn√©es volumineux pour l'entra√Ænement de mod√®les de reconnaissance.

Enfin, nous avons √©tabli un plan d'action clair, comprenant l'analyse approfondie des donn√©es disponibles, le pr√©traitement des images, et l'enrichissement du jeu de donn√©es par des techniques d'augmentation. Ces √©tapes pr√©liminaires posent les fondations n√©cessaires pour le d√©veloppement ult√©rieur de mod√®les de deep learning, qui seront essentiels pour la reconnaissance pr√©cise des champignons.

# 7. Annexe

1. [Scripts de webscraping, d'analyse du site Mushroom Observer et de s√©lection des donn√©s](https://github.com/DataScientest-Studio/jan24_cds_mushrooms/tree/main/notebooks/mushroom_observer)
2. [Script de s√©lection automatique avec YOLOv5](https://github.com/DataScientest-Studio/jan24_cds_mushrooms/tree/main/src/features/)
3. [Outil d'encadrement manuel MPBS](https://github.com/DataScientest-Studio/jan24_cds_mushrooms/tree/main/src/features/manual_picture_bbox_selector)
4. [Script d'oversampling](https://github.com/DataScientest-Studio/jan24_cds_mushrooms/tree/main/src/features)

# üçÑ Projet de groupe 2024 sur la reconnaissance de champignons üçÑ

> Auteurs : *[Heuzef](https://heuzef.com), Yvan Rolland, Viktoriia Saveleva, Florent Constant*

---

# Rendu N¬∞2 : Modelisation

> Date : *10/2024*

# 1. Introduction

Dans le cadre du projet de reconnaissance de champignons, nous abordons un probl√®me de deep learning qui s‚Äôapparente principalement √† une t√¢che de **classification**. 

L‚Äôobjectif est de classifier diff√©rentes esp√®ces de champignons en fonction de leurs caract√©ristiques visuelles, ce qui s‚Äôinscrit dans le domaine de la **reconnaissance d'image**. 

Pour √©valuer la performance des mod√®les d√©velopp√©s, nous utilisons principalement la m√©trique de **l'accuracy (pr√©cision)**, car elle permet de mesurer le pourcentage de classifications correctes effectu√©es. Cette m√©trique est particuli√®rement adapt√©e pour ce projet, car elle fournit une √©valuation claire et directe de l‚Äôefficacit√© du mod√®le √† distinguer les diff√©rentes esp√®ces de champignons.

Dans ce rapport, nous d√©crivons nos d√©marches, r√©flexions et erreurs. Nous analysons √©galement l'effet de la d√©t√©ction et de l'augmentation des donn√©es sur les r√©sultats de nos entrainements.

Un premier model na√Øf LeNet est utilis√© pour l'exp√©rimentation, puis finalement des algorithmes de transfert learning sont adopt√©s pour leur efficacit√©s.

La comparaison des r√©sultats est effectu√©e en introduisant la partie MLflow dans les algorithmes, ce qui nous permet de suivre la tra√ßabilit√© et de faciliter l'√©change et la comparaison des r√©sultats.

# 2. Pr√©-traitement des donn√©es

## Premi√®re approche

Pour rappel, le stockage des donn√©es se fait comme suit :

```
data
‚îú‚îÄ‚îÄ LAYER0
‚îÇ   ‚îú‚îÄ‚îÄ MO
‚îÇ       ‚îú‚îÄ‚îÄ MO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.csv
‚îÇ   ‚îî‚îÄ‚îÄ MO_106
‚îÇ       ‚îú‚îÄ‚îÄ MO_106
‚îÇ       ‚îú‚îÄ‚îÄ class_stats.csv
‚îÇ       ‚îú‚îÄ‚îÄ dispersion.csv
‚îÇ       ‚îî‚îÄ‚îÄ image_stats.csv
‚îú‚îÄ‚îÄ LAYER1
‚îÇ   ‚îî‚îÄ‚îÄ MO
‚îÇ       ‚îú‚îÄ‚îÄ MO
‚îÇ       ‚îú‚îÄ‚îÄ dataset.csv
‚îî‚îÄ‚îÄ LAYER2
    ‚îî‚îÄ‚îÄ MO
        ‚îú‚îÄ‚îÄ MO
        ‚îú‚îÄ‚îÄ dataset.csv
        ‚îî‚îÄ‚îÄ names.csv        
```

La r√©partion de ces donn√©es intervient dans le but pr√©cis de s'assurer de la qualit√© des donn√©s avant l'apprentissage pour optimiser les r√©sultats.

**LAYER0** : Obtenu par une s√©lection manuelle et un webscraping, ce qui nous a permis de constituer un dataset comportant 23 classes. L'objectif √©tait d'avoir au moins une centaine de photos par classe.
Dans le dossier MO, les photos extraites du site Mushroom Observer.
Dans le dossier MO_106, les photos extraites par Webscraping du site : https://www.mycodb.fr/  (utilis√©es uniquement pour des tests).

**LAYER1** : Lancement de la detection effectu√©e par YoloV5 (boxing), nous perdons environ 60% des images qui n'ont malheureusement pas √©t√© detect√©e par YoloV5. L'objectif est d'obtenir une base de donn√©e contenant des images de champignons les plus pr√©cises. La base de donn√©e √©tant l'√©l√©ment le plus important pour  l'apprentissage, il nous apparaissant pertinent de proc√©der par une d√©tection et un boxing, focus sur le champignon pour limiter le bruit.

![Boxing Error](./img/boxing_error.png)

Le traitement effectue √©galement des modifications sur l'image n√©cessaire au Deep Learning : redimensionnement en 224 X 224  px selon les coordonn√©es du rectangle de d√©tection.

**LAYER2** : Cr√©√© suite √† une augmentation des donn√©es.

Cela entra√Ænerait une pr√©cision excellente (>0.99) d√®s la premi√®re √©poque sans optimisation, ce que nous souhaitons √©viter. 

La s√©paration initiale garantie que les donn√©es d'entra√Ænement et de validation sont distinctes, permettant une √©valuation plus pr√©cise et une g√©n√©ralisation correcte du mod√®le.

En r√©sum√©, LAYER2 repr√©sente les donn√©es d'entra√Ænement augment√©es, tandis que les ensembles de validation et de test restent intacts et non modifi√©s pour une √©valuation juste et pr√©cise du mod√®le. La figure ci-dessous montre sch√©matiquement la structure des couches.

![Layers_structure_rapport2_v1.png](./img/Layers_structure_rapport2_v1.png)

## Deuxi√®me approche

Nos premiers essais montre des performances absolument incroyables, cependant, ceci s'explique par une erreur dans notre approche.

En effet, si nous proc√©dons d'abord par l'augmentation des donn√©es puis la division, les ensembles de validation contiendrons des images trop proches de l'entrainement, car simplement modifi√©es par l'augmentation des donn√©es.

√Ä ce stade, il est n√©cessaire d'effectuer l'inverse, en effectuant l'augmentation des donn√©s exclusivement sur le jeu d'entrainement divis√© en amont, sans toucher au jeu de validation et de test.

Notre nouvelle arboresence se pr√©sente donc ainsi :

```bash
data
‚îú‚îÄ‚îÄ LAYER0
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset.csv
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ MO
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1174
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 15162
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1540
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ (...)
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 939
‚îú‚îÄ‚îÄ LAYER1
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ MO
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1174
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 15162
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1540
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ (...)
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 939
‚îî‚îÄ‚îÄ LAYER2
    ‚îú‚îÄ‚îÄ MO
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 1174
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 15162
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 1540
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ (...)
 ¬†  ‚îÇ¬†¬† |   ‚îî‚îÄ‚îÄ 939
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ train
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 1174
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 15162
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 1540
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ (...)
 ¬†  ‚îÇ¬†¬† |   ‚îî‚îÄ‚îÄ 939
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ validation
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1174
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 15162
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 1540
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ (...)
 ¬†  ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 939
    ‚îî‚îÄ‚îÄ names.csv
```

Nous prenons √† ce stade la d√©cision de ne plus effectuer la detection des champignons via le model YoloV5 pour 2 raisons :

1. La quantit√©e d'image brutes perdues de 60% est trop importante.

2. Nous ne constatons pas d'am√©lioration des performances suite √† cette detection.

Les donn√©es de notre base de donn√©s "MO" seront divis√©es un jeu d'entrainement, de validation et de test (directement op√©r√© par le code mod√®le). 

Finalement, nos mod√®les entrain√©s seront √©valu√©s sur le jeu de test afin de les optimisers pour obtenir la meilleur pr√©cision possible.

![Layers_structure_rapport2_v2.png](./img/Layers_structure_rapport2_v2.png)

# 3. Algorithmes de Deep Learning s√©lectionn√©s et Optimisation

Les champignons pr√©sentent une diversit√© visuelle significative, avec des variations subtiles de forme, de couleur, et de texture. Les algorithmes de Deep Learning, notamment les **r√©seaux de neurones convolutifs (CNN)**, sont particuli√®rement efficaces pour extraire et apprendre des caract√©ristiques pertinentes √† partir d'images, ce qui en fait l'approche id√©ale pour identifier correctement les diff√©rentes esp√®ces de champignons. 

Une premi√®re exp√©rimentation est effectu√©e avec un mod√®le na√Øf **LeNet**, ce dernier permet d'obtenir des r√©sultats interessants, bien que moindre face aux m√©thodes de **Transfert learning**, qui offres des performances nettement sup√©rieurs.

En effet, c'est une strat√©gie cl√© dans notre approche. En utilisant des mod√®les pr√©-entra√Æn√©s sur de vastes ensembles de donn√©es comme ImageNet, nous avons pu adapter ces mod√®les √† notre probl√®me sp√©cifique de reconnaissance des champignons, ce qui a consid√©rablement am√©lior√© les performances des mod√®les.

Pour cette t√¢che, nous avons test√© plusieurs architectures : VGG16, EfficientNetB1, ResNet50.

Finalement, nous d√©cidons de concevoir un mod√®le avec une architecture sur mesure: JarviSpore. 
Ce dernier est partag√© sur HuggingFace: https://huggingface.co/YvanRLD

## 3.1. VGG16

VGG16 est une architecture de r√©seau de neurones convolutifs (CNN) d√©velopp√©e par l'√©quipe Visual Geometry Group de l'Universit√© d'Oxford. Ce mod√®le se distingue par sa structure simple et uniforme, compos√©e de 16 couches profondes dont 13 couches convolutives et 3 couches enti√®rement connect√©es. Gr√¢ce √† sa conception, VGG16 a d√©montr√© une grande efficacit√© dans la classification d'images, comme en t√©moigne ses performances remarquables lors de la comp√©tition ImageNet 2014. Dans le cadre du transfer learning, VGG16 est fr√©quemment utilis√© avec des poids pr√©entra√Æn√©s sur des ensembles de donn√©es √©tendus tels qu'ImageNet. Cette strat√©gie permet d'adapter les caract√©ristiques apprises par le mod√®le √† des t√¢ches sp√©cifiques avec des ensembles de donn√©es plus petits, offrant ainsi une pr√©cision accrue m√™me avec des donn√©es limit√©es.

![VGG16_layover.jpg](img/VGG16_layover.jpg)

Le "16" dans VGG16 fait r√©f√©rence aux 16 couches qui ont des poids. Dans VGG16, il y a treize couches convolutives, cinq couches de max pooling, et trois couches denses, ce qui fait un total de 21 couches, mais seulement seize couches avec des poids, c'est-√†-dire des couches avec des param√®tres apprenables.

VGG16 prend en entr√©e un tenseur de taille 224 x 224 px avec 3 canaux RGB. La particularit√© de VGG16 est qu‚Äôau lieu d'avoir un grand nombre d'hyperparam√®tres, le mod√®le se concentre sur des couches de convolution avec des filtres de 3x3 avec un stride de 1, en utilisant syst√©matiquement un padding constant et des couches de max pooling avec des filtres de 2x2 et un stride de 2.

Les couches de convolution et de max pooling sont dispos√©es de mani√®re coh√©rente tout au long de l'architecture. La couche Conv-1 poss√®de 64 filtres, Conv-2 en a 128, Conv-3 en a 256, et Conv-4 et Conv-5 ont 512 filtres chacun.

Trois couches enti√®rement connect√©es (FC) suivent une pile de couches convolutives : les deux premi√®res ont chacune 4096 canaux, et la troisi√®me effectue une classification ILSVRC √† 1000 classes, contenant ainsi 1000 canaux (un pour chaque classe). La couche finale est la couche softmax.

**Les entrainement r√©alis√©s**

De nombreux entrainements en transfert learning ont √©t√© r√©alis√©s en utilisant les poids imagenet disponibles directement via keras.
Ces entrainements ont permis d'exp√©rimenter deux architectures distinctes ainsi que l'influence de certains hyperparametres :

La premi√®re s√©rie d'essai a √©t√© r√©alis√©e avec une architecture comportant une seule couche dense pour la classification. 
Les exp√©rimentations men√©es consistaient √† faire varier le batch size et le taux d'augmentation des donn√©es.
Les r√©sultats semblent indiquer globalement que le batch size influence de mani√®re importante la precision du mod√®le. Dans les essais men√©s plus la taille du batch size √©tait importante, plus la pr√©cision √©tait grande. Les limite de m√©moire de la machine de test n'ont pas permis d'exp√©rimenter un batch size au del√† de 128.
Par ailleur on note que l'augmentation de donn√©es n'a quasiement aucune influence sur le r√©sultats.
Cette architecture a permis d'obtenir des scores compris entre  66% et 76% de pr√©cision.

La seconde serie d'essais a √©t√© r√©alis√©e sur une architecture comportant trois couches denses, comme le mod√®le vgg16 original mais entrecoup√©es de couches dropout. Les exp√©rimentations men√©es, en plus de faire varier le batch size et le taux d'augmentation des donn√©es, consistaient √† faire varier le nombre de cannaux et et le taux de dropout des diff√©rentes couches.

Les combinaisons suivantes ont √©t√© test√©es :

* Batch size 32 et 64

* Nombre de cannaux dans les 2 premi√®res couches de classification: 64,128,256,512,1024,2048

* Taux de dropout entre chaque couches: 0%, 10%, 20%

Cette architecture a permis d'obtenir des scores compris entre  70% et 77% de pr√©cision.

Ces entrainements ont r√©v√©l√© une tr√®s forte tendance du mod√®le √† faire du sur-apprentissage sur notre jeu de donn√©es, avec une pr√©cision sur les donn√©es de test rapidement √† 100% quelque soit l'architecture et les param√®tres employ√©s:

![newplot.png](img/newplot.png)

## 3.2. EfficientNetB1

### 3.2.1 Environnement de travail

L'entra√Ænement du model EfficientNetB1 a √©t√© r√©alis√© sur un environnement sans GPU, ce qui pose certaines contraintes en termes de performance (en particulier sur la m√©moire vive). Afin d'entra√Æner un mod√®le efficace malgr√© ces limitations, tout en optimisant l'usage des ressources disponibles.  Une astuce fournit par Google est utilis√© pour permettre un entrainement sur CPU, [en configurant la fonction exp√©rimental AutoShardPolicy](https://www.tensorflow.org/datasets/performances).

### 3.2.2 Pr√©paration √† la pr√©diction

Les labels des images ont √©t√© r√©cup√©r√©s √† partir de `.class_names`, fournissant ainsi une liste ordonn√©e des classes.

Une fonction Python personnalis√©e `get_champi_name()` a √©t√© utilis√©e pour organiser les noms des labels en fonction des besoins sp√©cifiques du projet pour la pr√©diction.

### 3.2.3 Entrainement du mod√®le

Les images d‚Äôentr√©e ont √©t√© redimensionn√©es en 224x224 pixels, conform√©ment aux attentes du mod√®le pr√©-entra√Æn√©. Ce format est commun√©ment utilis√© pour des mod√®les bas√©s sur **ImageNet**.

Le mod√®le utilis√© est pr√©-entra√Æn√© sur le dataset ImageNet, qui contient plus de 14 millions d'images et 1000 classes. Ce mod√®le, comprenant environ 9 millions de param√®tres, permet d‚Äôobtenir une base solide pour la classification des images.

Le `batch_size` est fix√© √† 32, un compromis entre la vitesse d‚Äôentra√Ænement et l‚Äôutilisation de la m√©moire vive. La structure du mod√®le a √©t√© adapt√©e pour int√©grer la couche finale de classification avec activation softmax, en fonction du nombre de classes cibles.

### 3.2.4 √âvaluation des r√©sultats

Sur 23  classes, le mod√®le a atteint une pr√©cision moyenne de 98 % lors de l'entra√Ænement, indiquant une forte capacit√© √† g√©n√©raliser sur les donn√©es d'entra√Ænement.

Les r√©sultats sur les donn√©es de validation varient entre 80 % et 90 %, en fonction des ajustements apport√©s aux hyperparam√®tres. Cela montre que le mod√®le a une bonne capacit√© de g√©n√©ralisation, mais pourrait √™tre affin√© pour √©viter le surapprentissage.

Certaines classes de champignion sont visiblement tr√®s probl√®matique avec ce mod√®le et ne parviennent quasiment jamais √† effectuer des pr√©dictions juste.

![efficientnetb1_matrice_01](./img/efficientnetb1_matrice_01.png)

Il est not√© que certaines esp√®ces de champignons, comme par exemple le Stropharia ambigua *(classe 14)* est souvent pr√©dite comme une autre espece, la seule nuance qui permette de diff√©rencier √©tant la couleur jaun√¢tre propre a cette esp√®ce, nous pouvons en d√©duire que ce mod√®le n'est pas  tr√®s performant sur la prise en compte des  nuances de couleurs.

![champi_diff](./img/champi_diff.png)

*Les Stropharia ambigua sont pr√©dites sans prendre en compte leur couleur jaun√¢tre.*

### 3.2.5 Optimisation

De nouveaux essais sont effectu√©es sur 16 classes uniquement pour volontairement exclure les classes probl√©matiques, avec une augmentation des donn√©s et un nombre d'epoch plus g√©n√©reux.

![dataset](./img/efficientnetb1_dataset.png)

Ajout de callbacks : **ReduceLROnPlateau** pour am√©liorer la d√©cente de gradient et **EarlyStopping** pour eviter le sur-entrainement.

Deux couches Dropout avec un  taux de 0.5 ont est ajout√©s au r√©seau pour le rendre plus robuste.

Les pr√©c√©dents r√©sultats montrent que les predictions sont clairement moins fiables sur les derni√®res classes. Ceci est caus√© car les donn√©es ne sont pas m√©lang√©s al√©atoirement sur les diff√©rents jeux de donn√©. Ainsi, un Shuffle est activ√© pour forcer l'entrainement des donn√©es dans un ordre al√©atoire.

L'entrainement s'arr√™te apr√®s seulement 4 epochs gr√¢ce au EarlyStopping, le sur-entrainement sur ce mod√®le interevient tr√®s rapidement de par sa nature, mais offre de bonnes perfomances.

![metrics](./img/efficientnetb1_metrics.png)

![matrix_02](./img/efficientnetb1_matrix_02.png)

![predictions](./img/efficientnetb1_predictions.png)

### 3.2.6 Conclusion

L'entra√Ænement du mod√®le EfficientNetB1 sur un environnement sans GPU a permis d'obtenir des r√©sultats satisfaisants malgr√© les limitations mat√©rielles. En optimisant l'utilisation des ressources, notamment gr√¢ce √† l'astuce de la configuration `AutoShardPolicy`, le mod√®le a pu tirer parti d'un environnement CPU tout en maintenant de bonnes performances.

L'utilisation d'un mod√®le pr√©-entra√Æn√© sur ImageNet fourni une base solide pour la classification. De plus, la gestion personnalis√©e des labels a permis une adaptation efficace aux besoins sp√©cifiques du projet. Nous constatons cependant que ce mod√®le n'est malheureusement pas tr√®s performant lorsqu'il s'agit de nuancer les couleurs des diff√©rentes √©sp√®ces.

Les performances du mod√®le ont montr√© une pr√©cision d'entra√Ænement remarquable √† 96% et une pr√©cision de validation de 86%.

Sur le jeu de test, les scores sont cependant plus interessants :

| Accuracy        | Precision       | Recall          | F1-score        |
| --------------- | --------------- | --------------- | --------------- |
| 0.9286764705882 | 0.9336224871829 | 0.9286764705882 | 0.9290201971718 |

Bien que ces r√©sultats soient encourageants, ils r√©v√®lent √©galement des marges de progression, notamment pour affiner les scores de pr√©cision sur le jeu d'√©valuation.

Ces conclusions ouvrent la voie √† des pistes d'am√©lioration, telles que l'optimisation des hyperparam√®tres et une meilleure gestion des donn√©es pour minimiser le risque de sur-apprentissage, EfficientNetB1 √©tant particuli√®rement senssible au sur-entrainement.

Bien que l'entrainement sur CPU est satisfaisant, effectuer ces exp√©rimentations avec un GPU devrais offrir un gain de vitesse.

## 3.3 ResNet50

Apr√®s avoir explor√© EfficientNetB1, nous avons d√©cid√© de tester ResNet50. Cette architecture se distingue par ses blocs r√©siduels qui facilitent l'entra√Ænement de r√©seaux tr√®s profonds en ajoutant des connexions directes entre les couches. Pour la reconnaissance des champignons, ResNet50 peut √™tre particuli√®rement int√©ressant en raison de sa capacit√© √† extraire des caract√©ristiques complexes tout en maintenant une efficacit√© computationnelle, ce qui est crucial pour des t√¢ches de classification fines comme celle-ci.

### 3.3.1. Mod√®le de base

Le mod√®le est bas√© sur **ResNet50**, pr√©-entra√Æn√© sur le jeu de donn√©es **ImageNet**. Nous avons enlev√© la partie sup√©rieure du mod√®le (le "top") pour adapter le r√©seau pr√©-entra√Æn√© √† notre t√¢che sp√©cifique de reconnaissance des champignons. La partie sup√©rieure d'un mod√®le pr√©-entra√Æn√© est g√©n√©ralement con√ßue pour des classes sp√©cifiques du jeu de donn√©es d'origine, comme ImageNet. En retirant cette partie, nous pouvons ajouter des couches adapt√©es √† notre propre ensemble de classes, ce qui permet au mod√®le de s'ajuster aux sp√©cificit√©s de notre t√¢che de classification multiclasse. Nous avons ajout√© une couche de **GlobalAveragePooling2D** suivie d'une couche **Dense** de 1024 neurones (taille couramment utilis√©e dans de nombreux r√©seaux de neurones pour les couches cach√©es) avec activation **ReLU**. La derni√®re couche de sortie est une couche **Dense** avec autant de neurones que de classes dans les donn√©es, utilisant une activation **softmax** pour la classification multiclasse.

Les couches du mod√®le pr√©-entra√Æn√© ResNet50 ont √©t√© gel√©es (non-entra√Ænables) pour conserver les poids appris pr√©c√©demment et √©viter de modifier ces param√®tres durant l'entra√Ænement. Le mod√®le a √©t√© compil√© avec l'optimiseur **Adam** et une faible valeur d'apprentissage (learning rate = 1e-4). La perte utilis√©e est **categorical crossentropy**, avec une m√©trique d‚Äô√©valuation sur la **pr√©cision**.

![resnet50_model1.png](img/resnet50_model1.png)

**R√©sultats** obtenus :

Pr√©cision d'entra√Ænement : Le mod√®le montre une pr√©cision qui commence √† 71 % et atteint presque 100 % (99,96 %) √† la fin de l‚Äôentra√Ænement. Cela montre que le mod√®le apprend tr√®s bien les donn√©es d‚Äôentra√Ænement, mais cela sugg√®re aussi un risque de **surapprentissage** (overfitting).

Pr√©cision de validation : La pr√©cision de validation commence relativement √©lev√©e √† 81 %, mais fluctue au fil des √©poques, se stabilisant autour de 84 %. Le mod√®le g√©n√©ralise relativement bien, mais ne montre pas d'am√©lioration significative apr√®s quelques it√©rations, sugg√©rant un plateau dans l'apprentissage.

Perte de validation : La perte de validation diminue l√©g√®rement au d√©but, mais √† partir de la cinqui√®me √©poque, elle commence √† augmenter. Cela refl√®te encore une fois un surapprentissage, car la perte d‚Äôentra√Ænement continue de baisser tandis que la perte de validation augmente. Cela signifie que le mod√®le se sp√©cialise trop sur les donn√©es d‚Äôentra√Ænement et ne parvient pas √† bien g√©n√©raliser sur de nouvelles donn√©es.

### 3.3.2. Mod√®les ajust√©s

1) **Ajout de Dropout (0.5)**
   Le Dropout a √©t√© ajout√© apr√®s la couche de GlobalAveragePooling2D et apr√®s la couche Dense, avec un taux de 0,5. Cela permet de r√©duire le surapprentissage (overfitting) en d√©sactivant al√©atoirement 50 % des neurones pendant l'entra√Ænement. Cela rend le mod√®le moins d√©pendant de certains neurones sp√©cifiques et am√©liore sa capacit√© de g√©n√©ralisation.
   
   **R√©gularisation L2 (0.001)**
   Une r√©gularisation L2 a √©t√© appliqu√©e sur la couche Dense. Cette technique p√©nalise les poids excessivement √©lev√©s, contribuant √† r√©duire le surapprentissage en encourageant des poids plus petits. Cela aide √† cr√©er un mod√®le plus stable et capable de mieux g√©n√©raliser aux nouvelles donn√©es.
   
   **R√©sultats** : 
   La pr√©cision d'entra√Ænement atteint 77 %, tandis que la pr√©cision de validation passe de 70 % √† 80 % avec une perte de validation en baisse constante, montrant que la r√©gularisation par Dropout et L2 aide √† mieux g√©n√©raliser et √† r√©duire le surapprentissage.

2) **Unfreezed layers**
   
   Les 10 derni√®res couches du mod√®le de base ResNet50 ont √©t√© "d√©fig√©es" pour √™tre entra√Ænables, ce qui permet √† ces couches d'affiner leurs poids pendant l'entra√Ænement. L'apprentissage est effectu√© avec un taux d'apprentissage plus bas (1e-5) pour √©viter une mise √† jour trop rapide des poids, et ce sur 10 √©poques.
   
   **R√©sultats** : Le surapprentissage est probable, car l'exactitude en entra√Ænement est tr√®s √©lev√©e, mais l'exactitude en validation stagne et la perte en validation ne diminue pas significativement.

3) **R√©gularisation et Dropout** :  Deux couches de Dropout √† 50% et la r√©gularisation L2 sont ajout√©es pour limiter le surapprentissage, en r√©duisant la d√©pendance du mod√®le √† certaines connexions sp√©cifiques.
   
   **Optimisation de l'entra√Ænement**
   **Early Stopping** : Le mod√®le arr√™te l'entra√Ænement si la perte en validation ne s'am√©liore plus apr√®s 3 √©poques, tout en restaurant les meilleurs poids, √©vitant un surapprentissage inutile.
   
   **R√©duction du taux d'apprentissage (ReduceLROnPlateau)** : Ce callback diminue progressivement le taux d'apprentissage si la validation stagne, permettant des ajustements plus fins dans les derni√®res √©tapes d'entra√Ænement.
   
   **Augmentation des epochs** : 
   Le nombre d'√©poques est pass√© √† 50, en combinaison avec les m√©canismes de contr√¥le du surapprentissage, pour permettre au mod√®le d'explorer un espace plus large de solutions tout en conservant une bonne g√©n√©ralisation.
   
   **R√©sultats** : L'exactitude d'entra√Ænement d√©passe 94%, mais l'exactitude de validation progresse lentement et se stabilise, indiquant un possible surapprentissage. La perte d'entra√Ænement diminue, tandis que la perte de validation augmente apr√®s quelques √©poques, renfor√ßant l'hypoth√®se de surapprentissage. Le taux d'apprentissage, initialement √† 1e-5, a √©t√© r√©duit √† 2e-6 apr√®s la 7e √©poque, montrant que le mod√®le a atteint un plateau t√¥t.

4) **Augmentation du taux de dropout** √† 0,7 pour r√©duire la d√©pendance excessive aux neurones sp√©cifiques et pr√©venir le surapprentissage. 5 couches de ResNet50 sont d√©sormais d√©gel√©es pour affiner davantage l'apprentissage. 
   Le **callback EarlyStopping** a une patience augment√©e √† 5 pour permettre au mod√®le de continuer l'entra√Ænement plus longtemps avant d'arr√™ter si la validation ne s'am√©liore pas, et le **ReduceLROnPlateau** r√©duit le taux d'apprentissage plus progressivement, avec un plancher fix√© √† 1e-6.
   Augmentation de la **r√©gularisation L2** dans la couche de sortie √† 0.01 pour mieux contr√¥ler le surapprentissage.

![resnet50_model_last.png](img/resnet50_model_last.png)

**R√©sultats** L'exactitude d'entra√Ænement atteint 95%, mais la pr√©cision de validation stagne autour de 80%, sugg√©rant un surapprentissage. La perte de validation diminue au d√©but mais augmente ensuite, ce qui confirme √©galement un surapprentissage. Le taux d'apprentissage r√©duit consid√©rablement apr√®s la 12e √©poque, montrant que le mod√®le a atteint un plateau pr√©coce dans l'entra√Ænement.

### Conclusion

Malgr√© une haute pr√©cision d'entra√Ænement (95%), le mod√®le montre encore du **surapprentissage**, avec une pr√©cision de validation stagnante autour de 80% et une perte de validation croissante.

Causes possibles :

- Le mod√®le pr√©-entra√Æn√© sur ImageNet peut ne pas capturer suffisamment les caract√©ristiques sp√©cifiques des champignons.
- Les donn√©es d'entra√Ænement pourraient √™tre insuffisantes pour une g√©n√©ralisation efficace.
- Le taux d'apprentissage pourrait ne pas √™tre optimal pour ce probl√®me sp√©cifique.

Prochaines √©tapes :

- Augmenter la taille et la diversit√© des donn√©es d'entra√Ænement sur les champignons.
- Ajuster ou simplifier l'architecture du mod√®le.
- Ajouter des techniques de r√©gularisation suppl√©mentaires.
- Exp√©rimenter avec des mod√®les sp√©cifiquement entra√Æn√©s sur des donn√©es de champignons ou utiliser des techniques de transfert learning adapt√©es.

## 3.4. ResNet18

ResNet18 est comme ResNet50 un r√©seau de neurones convolutifs utilisant des connexions r√©siduelles, mais avec une profondeur de 18 couches seulement.

Les essais men√©s avec ResNet18 ont √©t√© imm√©diatement concluants avec un score de pr√©cision d'environs 97% sur notre dataset (23 classe avec en moyenne 166 images par classe).

![resnet18_01](./img/resnet18_01.png)

Diff√©rents essais on permis d'estimer le nombre d'images necessaires pour obtenir un niveau de pr√©cision satsfaisant. Ainsi on constate qu'avec un dataset comprenant seulement 80 images par classe on atteint une pr√©cision de 97%. Avec seulement 30 images par classe on reste au dessus de 90% de pr√©cision, et m√™me avec seulement 10 images par classe on reste au dessus de 80% de pr√©cision.

![resnet18_02](./img/resnet18_02.png)

## 3.5 JarviSpore

Suite aux r√©sultats mitig√©s sur le transfert Learning, nous avons pris l'initiative de cr√©er un mod√®le de z√©ro avec comme support les cours DataScientest et des livres.

Nous avons investi dans un PC avec carte graphique RTX 3090 disposant de 24 Go de VRAM. Notre PC dispose de 192 Go de RAM. Processeur i9 14900k.

Nous avons ensuite pris le parti de tester dans un environnement WSL2. Cela nous permettait d'utiliser les derni√®res versions de TensorFlow, Cuda, Cudnn et Keras.

Apr√®s l'installation, nous avons construit le mod√®le dans VSCode, mais lors des entra√Ænements, des probl√®mes de m√©moire nous ont compliqu√©s la t√¢che.

Nous avons d√©ploy√© un environnement sous Windows en utilisant d'anciennes versions de TensorFlow, Cuda ‚Ä¶

Pour assurer la compatibilit√© des biblioth√®ques utilis√©es et de leurs versions, car la compatibilit√© TensorFlow sous Windows s'arr√™te √† la version 2.10 :

```
numpy : 1.26.4
tensorflow : 2.10.0
matplotlib : 3.9.2
scikit-learn : 1.5.2
PIL : 10.4.0
cv2 : 4.10.0
pandas : 2.2.3
```

Ce mod√®le effectue l'entra√Ænement, l'√©valuation et l'interpr√©tation d'un mod√®le de r√©seau de neurones convolutif (CNN) pour une t√¢che de classification d'images. Voici les diff√©rentes √©tapes et le processus utilis√©s :

1. Importation des Biblioth√®ques  
   Nous commen√ßons par importer les biblioth√®ques n√©cessaires pour la manipulation des donn√©es, l'entra√Ænement du mod√®le, l'√©valuation et la visualisation des r√©sultats. Les biblioth√®ques incluent TensorFlow pour la construction du mod√®le, NumPy pour les calculs num√©riques, Pandas pour la gestion des donn√©es et OpenCV pour le traitement des images.

2. Extraction des Versions des Biblioth√®ques  
   Nous v√©rifions les versions des biblioth√®ques utilis√©es afin d'assurer la compatibilit√© des versions.

3. Chargement des Datasets (structur√©es et non structur√©es)  
   Nous d√©finissons les chemins pour les datasets d'entra√Ænement, de validation et de test. Nous utilisons la fonction image_dataset_from_directory pour charger les images en les redimensionnant √† la taille (224, 224) avec un batch size de 32 images. Les ensembles de donn√©es sont ensuite configur√©s pour √™tre mis en cache en m√©moire vive, pr√©charg√©s et optimis√©s.

4. Chargement des Classes  
   Nous chargeons les noms des classes √† partir d'un fichier CSV (API MushroomObserver) pour obtenir la liste des classes disponibles. Cela permet au mod√®le d'associer les indices des classes avec les noms r√©els lors de l'affichage des r√©sultats.

5. Construction du Mod√®le Convolutionnel  
   Nous construisons un CNN personnalis√© avec plusieurs couches de convolution suivies de la normalisation par lots (Batch Normalization), du sous-√©chantillonnage (MaxPooling) et d'une couche de sortie utilisant softmax pour la classification des 23 classes. Les couches de convolution permettent d'extraire les caract√©ristiques des images, tandis que les couches denses √† la fin effectuent la classification.

6. Compilation du Mod√®le  
   Le mod√®le est compil√© avec l'optimiseur Adam et la fonction de perte sparse_categorical_crossentropy, adapt√©e √† la classification multi-classes avec des √©tiquettes sous forme d'entiers.

7. Ajout de l'Early Stopping et du Model Checkpoint  
   Nous configurons des callbacks pour arr√™ter l'entra√Ænement si la pr√©cision de validation n'augmente plus apr√®s 5 √©poques (early stopping) et pour sauvegarder le meilleur mod√®le lors de l'entra√Ænement (ModelCheckpoint).

8. Gestion du D√©s√©quilibre des Classes  
   Nous v√©rifions le d√©s√©quilibre des classes dans l'ensemble d'entra√Ænement. Si certaines classes sont moins repr√©sent√©es, nous utilisons des pond√©rations de classe (class_weight) pour accorder plus d'importance aux classes sous-repr√©sent√©es afin d'am√©liorer la g√©n√©ralisation du mod√®le.

9. Entra√Ænement du Mod√®le  
   Le mod√®le est entra√Æn√© sur 20 √©poques, en utilisant les pond√©rations de classe pour mieux g√©rer les d√©s√©quilibres. Les callbacks configur√©s permettent de surveiller la performance et de sauvegarder le meilleur mod√®le.

10. G√©n√©ration de la Matrice de Confusion  
    Apr√®s l'entra√Ænement, nous g√©n√©rons une matrice de confusion sur l'ensemble de validation pour √©valuer la capacit√© du mod√®le √† classifier correctement les images. La matrice de confusion est affich√©e avec les noms des classes pour faciliter l'interpr√©tation des r√©sultats.

11. Visualisation des Courbes d'Entra√Ænement  
    Nous affichons les courbes de pr√©cision et de perte pour les ensembles d'entra√Ænement et de validation, ce qui nous permet de visualiser l'√©volution des performances du mod√®le pendant l'entra√Ænement.

12. Sauvegarde du Mod√®le et M√©tadonn√©es  
    Nous sauvegardons le mod√®le entra√Æn√© au format .keras ainsi que les m√©tadonn√©es (date d'entra√Ænement, pr√©cision sur l'ensemble de test, nombre d'√©poques). Cela permet de documenter le mod√®le pour un suivi ult√©rieur.

13. Test et √âvaluation du Mod√®le sur l'Ensemble de Test  
    Nous testons le mod√®le sur le jeu de donn√©es de test pour obtenir la pr√©cision finale et √©valuer sa performance g√©n√©rale.

14. Affichage Grad-CAM  
    Nous impl√©mentons Grad-CAM pour visualiser les activations des couches de convolution du mod√®le. Cette technique permet d'afficher les r√©gions de l'image qui ont le plus contribu√© √† la d√©cision du mod√®le. Les r√©sultats sont affich√©s pour cinq images al√©atoires du jeu de test.

R√©sultats Attendues  

- Pr√©cision du Mod√®le : La m√©trique mesur√©e est la pr√©cision, elle permet de mesurer le pourcentage de classifications correctes effectu√©es.  
- Interpr√©tabilit√© avec Grad-CAM : Les heatmaps g√©n√©r√©es par Grad-CAM doivent indiquer les parties pertinentes de l'image, ce qui aide √† comprendre le fonctionnement du mod√®le.  
- G√©n√©ralisation : Avec l'utilisation des callbacks et des pond√©rations de classe, le mod√®le doit √©viter le sur-apprentissage et bien g√©n√©raliser sur les donn√©es de validation et de test.

Ces √©tapes permettent de construire un mod√®le performant pour la classification d'images, tout en prenant en compte les d√©s√©quilibres de classe et en offrant des outils d'interpr√©tation des r√©sultats.

Lien vers le mod√®le sur Hugging Face : https://huggingface.co/YvanRLD/JarviSpore

![jarvispore_001](./img/jarvispore_001.png)
![jarvispore_002](./img/jarvispore_002.png)
![jarvispore_002](./img/jarvispore_003.png)

# 4. Interpr√©tation des r√©sultats avec Grad-CAM

Pour mieux comprendre les r√©sultats et les d√©cisions prises par les algorithmes, nous avons utilis√© **Grad-CAM** (Gradient-weighted Class Activation Mapping), une technique puissante d'interpr√©tation des mod√®les de deep learning, en particulier pour la classification d'images. Cette m√©thode permet de visualiser les r√©gions d'une image qui influencent le plus les d√©cisions d'un mod√®le. En g√©n√©rant des cartes thermiques (heatmaps) superpos√©es sur les images d'entr√©e, Grad-CAM met en √©vidence les caract√©ristiques jug√©es essentielles par le mod√®le pour ses pr√©dictions.

Pour cr√©er ces cartes thermiques, on commence par calculer les gradients associ√©s √† la classe pr√©dite, en les reliant aux cartes de caract√©ristiques issues de la derni√®re couche de convolution. Ces gradients sont ensuite moyenn√©s pour obtenir une vue d'ensemble, qui sert √† ajuster les cartes de caract√©ristiques, mettant ainsi en lumi√®re les zones les plus importantes pour la classification.

Avec Grad-CAM, nous pouvons mieux comprendre les performances de nos mod√®les en analysant visuellement leurs points d'attention. Cette approche nous aide √† identifier les forces et les faiblesses des mod√®les, √† d√©celer d'√©ventuels biais et √† approfondir notre compr√©hension des d√©cisions prises par les algorithmes.

Le graphique ci-dessous illustre des exemples de cartes thermiques obtenues via EfficientNetB1 et ResNet50 pour quatre classes diff√©rentes de champignons.

 ![gradcam.png](./img/gradcam.png)

Les "zones chaudes" (zones rouges et jaunes des cartes thermiques) indiquent les r√©gions sur lesquelles le mod√®le se concentre. En g√©n√©ral, ces zones chaudes correspondent √† certaines parties du champignon, mais la zone de concentration varie selon la classe de champignon (par exemple, la tige par rapport √† la t√™te du champignon, le bord de la t√™te, etc.). Il est int√©ressant de noter que, pour l'image contenant deux champignons, ResNet50 performe mieux en identifiant les deux, tandis qu'EfficientNet se concentre principalement sur un seul champignon.
Cependant, pour la photo avec la pr√©sence de la main, ResNet50 √©tait compl√®tement perdu et ne se concentrait pas du tout sur le champignon, tandis qu'EfficientNet l'identifiait mieux. 

En somme, ces r√©sultats soulignent l'importance d'une analyse approfondie pour mieux comprendre les performances de chaque mod√®le dans des situations vari√©es.

# 5. Conclusion

## 5.1 Comparaison des resultats

Les diff√©rents essais r√©alis√©s ont mis en √©vidence d'importantes diff√©rences de r√©sultats obtenus avec divers mod√®les sur un m√™me jeu de donn√©es. Alors que certains mod√®les, comme VGG16, affichent des limites significatives pour ce cas d'utilisation, d'autres, tels que ResNet18, ont d√©montr√© d'excellentes performances.

En poursuivant notre analyse, nous avons √©galement compar√© ResNet18 et ResNet50. Cette comparaison montre qu'un mod√®le plus profond n'est pas toujours synonyme de meilleures performances ; au contraire, sur un petit jeu de donn√©es, un mod√®le plus complexe peut s'av√©rer moins performant.

Dans le cadre de notre projet, nous avons int√©gr√© l'approche MLflow pour am√©liorer le suivi et la gestion de nos exp√©riences en apprentissage automatique.

Ce dernier est utilis√© pour tracer chaque √©tape du processus exp√©rimental, notamment les param√®tres, les m√©triques, et les artefacts des mod√®les.

Nous avons configur√© un serveur de tracking, d√©fini un projet sp√©cifique pour organiser nos exp√©rimentions.

Cette int√©gration permet de centraliser et comparer les m√©triques et de faciliter le d√©ploiement ult√©rieur des mod√®les retenus.

Ainsi, nous pouvons suivre de mani√®re syst√©matique et efficace les progr√®s r√©alis√©s dans notre projet. Les captures suivantes r√©sument donc les r√©sultats de l'ensemble du projet :

![mlflow](./img/mlflow.png)
![mlflow2](./img/mlflow2.png)

## 5.2 Interpretabilit√©

Les diff√©rentes √©valuation des mod√®les effectu√©s sur des donn√©es de tests montrent que de fa√ßon global, les mod√®les ont tous tendance √† effectuer de la sur-interpr√©tation et g√®rent particuli√®rement mal les couleurs des champignons. 

En effet les m√©thodes Grad-Cam permettent de visualiser cette tendance √† prendre en compte des zones pr√©cises, sans se concentrer sur les zones tr√®s color√©s. La couleur est pourtant l'un des points les plus importants, les mod√®les montrent tous de grandes faiblesse pour diff√©rencier deux champignons physiquement identique avec la couleur comme seul √©l√©ment de diff√©renciation ou encore de simplement localiser un champigon, m√™me de couleur vive, si le fond de la photo contient des √©l√©ments avec une forte luminosit√© proche du blanc.

![jarvispore_004](./img/jarvispore_004.png)

## 5.3 Technique

Nous pouvons noter que si les mod√®les avec une architeture les plus basiques (Lenet) offre des resultats tr√®s moyen, ceux en transfert learning offrent cependant des resultats performants m√™me si rapidement sujet √† un sur-apprentissage malgr√®s nos essais avec diff√©rentes technique d'optimisation.

Nous concluons sur le fait que la conception d'un mod√®le sur-mesure, avec une architecture complexe, bien que tr√®s fastidieux, permet d'obtenir des m√©triques et rapports de classification plus performant √† tout les niveaux.

En effet, d√©cision est prise d'impl√©menter un mod√®le nomm√© JarviSpore, solution modulable au fur et √† mesure de l'avanc√©e de nos connaissances. Celui-ci est arriv√© √† maturit√© et pr√©sente des performances sup√©rieures.

# 6. Pour aller plus loin

## Mod√®les Open-Source

L'utilisation de mod√®les plus r√©cents accessible sur HuggingFace nous permettrais tr√®s surement  d'obtenir encore de meilleurs performance de pr√©cision.

Nous exp√©rimentons l'utilisation d'un mod√®le communautaire de classification pr√©-entrain√© sur 100 esp√®ces de champignons russe.

Ce mod√®le, partag√© par Dmytro Iakubovskyi, est entrain√© sur 233480 images et se base sur l'architecture ViT (85.9M de param√®tres).

[ü§ó Mid-ViT par dima806 - HuggingFace](https://huggingface.co/dima806/mushrooms_image_detection)

## R√©seaux Kolmogorov-Arnold

Les MLP (r√©seaux de neurones multicouches), bien qu'utilis√©s dans de nombreux contextes, sont souvent sujets √† l'overfitting en raison de leur grande flexibilit√©, et comportent de nombreux param√®tres difficiles √† interpr√©ter, ce qui limite leur utilit√© dans certaines applications. 

R√©cemment, les r√©seaux Kolmogorov-Arnold (KAN) ont √©t√© propos√©s comme une alternative prometteuse (article : https://arxiv.org/abs/2404.19756, GitHub : https://github.com/KindXiaoming/pykan). 

Contrairement aux MLP, qui utilisent des fonctions non lin√©aires fixes comme ReLU ou Tanh, les KAN exploitent des B-splines, des polyn√¥mes par morceaux, pour mod√©liser les donn√©es de mani√®re plus souple et ajust√©e. Cela permet d'am√©liorer l'interpr√©tabilit√© des mod√®les et de r√©duire le nombre de param√®tres, rendant les KAN plus efficaces et potentiellement moins sensibles √† l'overfitting.

Cependant, bien que les KAN pr√©sentent de nombreux avantages th√©oriques, ils restent instables, avec des r√©sultats sensibles aux hyperparam√®tres choisis, ce qui n√©cessite des ajustements soigneux pour chaque t√¢che. 

Pour les prochains tests, il sera crucial d'explorer davantage cette nouvelle architecture, de tester son potentiel de g√©n√©ralisation sur des donn√©es vari√©es, et d'√©valuer dans quelle mesure elle peut remplacer les MLP dans des architectures complexes.
