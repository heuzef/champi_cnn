# ğŸ„ Projet de groupe 2024 sur la reconnaissance de champignons ğŸ„

> Auteurs : *[Heuzef](https://heuzef.com), Yvan Rolland, Viktoriia Saveleva, Florent Constant*

---

# Rendu NÂ°2 : Modelisation

> Date : *10/2024*

# 1. Introduction

Dans le cadre du projet de reconnaissance de champignons, nous abordons un problÃ¨me de deep learning qui sâ€™apparente principalement Ã  une tÃ¢che de **classification**. 

Lâ€™objectif est de classifier diffÃ©rentes espÃ¨ces de champignons en fonction de leurs caractÃ©ristiques visuelles, ce qui sâ€™inscrit dans le domaine de la **reconnaissance d'image**. 

Pour Ã©valuer la performance des modÃ¨les dÃ©veloppÃ©s, nous utilisons principalement la mÃ©trique de **l'accuracy (prÃ©cision)**, car elle permet de mesurer le pourcentage de classifications correctes effectuÃ©es. Cette mÃ©trique est particuliÃ¨rement adaptÃ©e pour ce projet, car elle fournit une Ã©valuation claire et directe de lâ€™efficacitÃ© du modÃ¨le Ã  distinguer les diffÃ©rentes espÃ¨ces de champignons.

Dans ce rapport, nous dÃ©crivons nos dÃ©marches, rÃ©flexions et erreurs. Nous analysons Ã©galement l'effet de la dÃ©tÃ©ction et de l'augmentation des donnÃ©es sur les rÃ©sultats de nos entrainements.

Un premier model naÃ¯f LeNet est utilisÃ© pour l'expÃ©rimentation, puis finalement des algorithmes de transfert learning sont adoptÃ©s pour leur efficacitÃ©s.

La comparaison des rÃ©sultats est effectuÃ©e en introduisant la partie MLflow dans les algorithmes, ce qui nous permet de suivre la traÃ§abilitÃ© et de faciliter l'Ã©change et la comparaison des rÃ©sultats.

# 2. PrÃ©-traitement des donnÃ©es

## PremiÃ¨re approche

Pour rappel, le stockage des donnÃ©es se fait comme suit :

```
data
â”œâ”€â”€ LAYER0
â”‚   â”œâ”€â”€ MO
â”‚       â”œâ”€â”€ MO
â”‚   â”‚   â””â”€â”€ dataset.csv
â”‚   â””â”€â”€ MO_106
â”‚       â”œâ”€â”€ MO_106
â”‚       â”œâ”€â”€ class_stats.csv
â”‚       â”œâ”€â”€ dispersion.csv
â”‚       â””â”€â”€ image_stats.csv
â”œâ”€â”€ LAYER1
â”‚   â””â”€â”€ MO
â”‚       â”œâ”€â”€ MO
â”‚       â”œâ”€â”€ dataset.csv
â””â”€â”€ LAYER2
    â””â”€â”€ MO
        â”œâ”€â”€ MO
        â”œâ”€â”€ dataset.csv
        â””â”€â”€ names.csv        
```

La rÃ©partion de ces donnÃ©es intervient dans le but prÃ©cis de s'assurer de la qualitÃ© des donnÃ©s avant l'apprentissage pour optimiser les rÃ©sultats.

**LAYER0** : Obtenu par une sÃ©lection manuelle et un webscraping, ce qui nous a permis de constituer un dataset comportant 23 classes. L'objectif Ã©tait d'avoir au moins une centaine de photos par classe.
Dans le dossier MO, les photos extraites du site Mushroom Observer.
Dans le dossier MO_106, les photos extraites par Webscraping du site : https://www.mycodb.fr/  (utilisÃ©es uniquement pour des tests).

**LAYER1** : Lancement de la detection effectuÃ©e par YoloV5 (boxing), nous perdons environ 60% des images qui n'ont malheureusement pas Ã©tÃ© detectÃ©e par YoloV5. L'objectif est d'obtenir une base de donnÃ©e contenant des images de champignons les plus prÃ©cises. La base de donnÃ©e Ã©tant l'Ã©lÃ©ment le plus important pour  l'apprentissage, il nous apparaissant pertinent de procÃ©der par une dÃ©tection et un boxing, focus sur le champignon pour limiter le bruit.

![Boxing Error](./img/boxing_error.png)

Le traitement effectue Ã©galement des modifications sur l'image nÃ©cessaire au Deep Learning : redimensionnement en 224 X 224  px selon les coordonnÃ©es du rectangle de dÃ©tection.

**LAYER2** : CrÃ©Ã© suite Ã  une augmentation des donnÃ©es.

Cela entraÃ®nerait une prÃ©cision excellente (>0.99) dÃ¨s la premiÃ¨re Ã©poque sans optimisation, ce que nous souhaitons Ã©viter. 

La sÃ©paration initiale garantie que les donnÃ©es d'entraÃ®nement et de validation sont distinctes, permettant une Ã©valuation plus prÃ©cise et une gÃ©nÃ©ralisation correcte du modÃ¨le.

En rÃ©sumÃ©, LAYER2 reprÃ©sente les donnÃ©es d'entraÃ®nement augmentÃ©es, tandis que les ensembles de validation et de test restent intacts et non modifiÃ©s pour une Ã©valuation juste et prÃ©cise du modÃ¨le. La figure ci-dessous montre schÃ©matiquement la structure des couches.

![Layers_structure_rapport2_v1.png](./img/Layers_structure_rapport2_v1.png)

## DeuxiÃ¨me approche

Nos premiers essais montre des performances absolument incroyables, cependant, ceci s'explique par une erreur dans notre approche.

En effet, si nous procÃ©dons d'abord par l'augmentation des donnÃ©es puis la division, les ensembles de validation contiendrons des images trop proches de l'entrainement, car simplement modifiÃ©es par l'augmentation des donnÃ©es.

Ã€ ce stade, il est nÃ©cessaire d'effectuer l'inverse, en effectuant l'augmentation des donnÃ©s exclusivement sur le jeu d'entrainement divisÃ© en amont, sans toucher au jeu de validation et de test.

Notre nouvelle arboresence se prÃ©sente donc ainsi :

```bash
data
â”œâ”€â”€ LAYER0
â”‚Â Â  â”œâ”€â”€ dataset.csv
â”‚Â Â  â””â”€â”€ MO
â”‚Â Â      â”œâ”€â”€ 1174
â”‚Â Â      â”œâ”€â”€ 15162
â”‚Â Â      â”œâ”€â”€ 1540
â”‚Â Â      â”œâ”€â”€ (...)
â”‚Â Â      â””â”€â”€ 939
â”œâ”€â”€ LAYER1
â”‚Â Â  â””â”€â”€ MO
â”‚Â Â      â”œâ”€â”€ 1174
â”‚Â Â      â”œâ”€â”€ 15162
â”‚Â Â      â”œâ”€â”€ 1540
â”‚Â Â      â”œâ”€â”€ (...)
â”‚Â Â      â””â”€â”€ 939
â””â”€â”€ LAYER2
    â”œâ”€â”€ MO
    â”‚Â Â  â”œâ”€â”€ test
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1174
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 15162
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1540
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ (...)
 Â   â”‚Â Â  |   â””â”€â”€ 939
    â”‚Â Â  â”œâ”€â”€ train
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1174
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 15162
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1540
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ (...)
 Â   â”‚Â Â  |   â””â”€â”€ 939
    â”‚Â Â  â””â”€â”€ validation
    â”‚Â Â      â”œâ”€â”€ 1174
    â”‚Â Â      â”œâ”€â”€ 15162
    â”‚Â Â      â”œâ”€â”€ 1540
    â”‚Â Â      â”œâ”€â”€ (...)
 Â   â”‚Â Â      â””â”€â”€ 939
    â””â”€â”€ names.csv
```

Nous prenons Ã  ce stade la dÃ©cision de ne plus effectuer la detection des champignons via le model YoloV5 pour 2 raisons :

1. La quantitÃ©e d'image brutes perdues de 60% est trop importante.

2. Nous ne constatons pas d'amÃ©lioration des performances suite Ã  cette detection.

Les donnÃ©es de notre base de donnÃ©s "MO" seront divisÃ©es un jeu d'entrainement, de validation et de test (directement opÃ©rÃ© par le code modÃ¨le). 

Finalement, nos modÃ¨les entrainÃ©s seront Ã©valuÃ©s sur le jeu de test afin de les optimisers pour obtenir la meilleur prÃ©cision possible.

![Layers_structure_rapport2_v2.png](./img/Layers_structure_rapport2_v2.png)

# 3. Algorithmes de Deep Learning sÃ©lectionnÃ©s et Optimisation

Les champignons prÃ©sentent une diversitÃ© visuelle significative, avec des variations subtiles de forme, de couleur, et de texture. Les algorithmes de Deep Learning, notamment les **rÃ©seaux de neurones convolutifs (CNN)**, sont particuliÃ¨rement efficaces pour extraire et apprendre des caractÃ©ristiques pertinentes Ã  partir d'images, ce qui en fait l'approche idÃ©ale pour identifier correctement les diffÃ©rentes espÃ¨ces de champignons. 

Une premiÃ¨re expÃ©rimentation est effectuÃ©e avec un modÃ¨le naÃ¯f **LeNet**, ce dernier permet d'obtenir des rÃ©sultats interessants, bien que moindre face aux mÃ©thodes de **Transfert learning**, qui offres des performances nettement supÃ©rieurs.

En effet, c'est une stratÃ©gie clÃ© dans notre approche. En utilisant des modÃ¨les prÃ©-entraÃ®nÃ©s sur de vastes ensembles de donnÃ©es comme ImageNet, nous avons pu adapter ces modÃ¨les Ã  notre problÃ¨me spÃ©cifique de reconnaissance des champignons, ce qui a considÃ©rablement amÃ©liorÃ© les performances des modÃ¨les.

Pour cette tÃ¢che, nous avons testÃ© plusieurs architectures : VGG16, EfficientNetB1, ResNet50.

Finalement, nous dÃ©cidons de concevoir un modÃ¨le avec une architecture sur mesure: JarviSpore. 
Ce dernier est partagÃ© sur HuggingFace: https://huggingface.co/YvanRLD

## 3.1. VGG16

VGG16 est une architecture de rÃ©seau de neurones convolutifs (CNN) dÃ©veloppÃ©e par l'Ã©quipe Visual Geometry Group de l'UniversitÃ© d'Oxford. Ce modÃ¨le se distingue par sa structure simple et uniforme, composÃ©e de 16 couches profondes dont 13 couches convolutives et 3 couches entiÃ¨rement connectÃ©es. GrÃ¢ce Ã  sa conception, VGG16 a dÃ©montrÃ© une grande efficacitÃ© dans la classification d'images, comme en tÃ©moigne ses performances remarquables lors de la compÃ©tition ImageNet 2014. Dans le cadre du transfer learning, VGG16 est frÃ©quemment utilisÃ© avec des poids prÃ©entraÃ®nÃ©s sur des ensembles de donnÃ©es Ã©tendus tels qu'ImageNet. Cette stratÃ©gie permet d'adapter les caractÃ©ristiques apprises par le modÃ¨le Ã  des tÃ¢ches spÃ©cifiques avec des ensembles de donnÃ©es plus petits, offrant ainsi une prÃ©cision accrue mÃªme avec des donnÃ©es limitÃ©es.

![VGG16_layover.jpg](img/VGG16_layover.jpg)

Le "16" dans VGG16 fait rÃ©fÃ©rence aux 16 couches qui ont des poids. Dans VGG16, il y a treize couches convolutives, cinq couches de max pooling, et trois couches denses, ce qui fait un total de 21 couches, mais seulement seize couches avec des poids, c'est-Ã -dire des couches avec des paramÃ¨tres apprenables.

VGG16 prend en entrÃ©e un tenseur de taille 224 x 224 px avec 3 canaux RGB. La particularitÃ© de VGG16 est quâ€™au lieu d'avoir un grand nombre d'hyperparamÃ¨tres, le modÃ¨le se concentre sur des couches de convolution avec des filtres de 3x3 avec un stride de 1, en utilisant systÃ©matiquement un padding constant et des couches de max pooling avec des filtres de 2x2 et un stride de 2.

Les couches de convolution et de max pooling sont disposÃ©es de maniÃ¨re cohÃ©rente tout au long de l'architecture. La couche Conv-1 possÃ¨de 64 filtres, Conv-2 en a 128, Conv-3 en a 256, et Conv-4 et Conv-5 ont 512 filtres chacun.

Trois couches entiÃ¨rement connectÃ©es (FC) suivent une pile de couches convolutives : les deux premiÃ¨res ont chacune 4096 canaux, et la troisiÃ¨me effectue une classification ILSVRC Ã  1000 classes, contenant ainsi 1000 canaux (un pour chaque classe). La couche finale est la couche softmax.

**Les entrainement rÃ©alisÃ©s**

De nombreux entrainements en transfert learning ont Ã©tÃ© rÃ©alisÃ©s en utilisant les poids imagenet disponibles directement via keras.
Ces entrainements ont permis d'expÃ©rimenter deux architectures distinctes ainsi que l'influence de certains hyperparametres :

La premiÃ¨re sÃ©rie d'essai a Ã©tÃ© rÃ©alisÃ©e avec une architecture comportant une seule couche dense pour la classification. 
Les expÃ©rimentations menÃ©es consistaient Ã  faire varier le batch size et le taux d'augmentation des donnÃ©es.
Les rÃ©sultats semblent indiquer globalement que le batch size influence de maniÃ¨re importante la precision du modÃ¨le. Dans les essais menÃ©s plus la taille du batch size Ã©tait importante, plus la prÃ©cision Ã©tait grande. Les limite de mÃ©moire de la machine de test n'ont pas permis d'expÃ©rimenter un batch size au delÃ  de 128.
Par ailleur on note que l'augmentation de donnÃ©es n'a quasiement aucune influence sur le rÃ©sultats.
Cette architecture a permis d'obtenir des scores compris entre  66% et 76% de prÃ©cision.

La seconde serie d'essais a Ã©tÃ© rÃ©alisÃ©e sur une architecture comportant trois couches denses, comme le modÃ¨le vgg16 original mais entrecoupÃ©es de couches dropout. Les expÃ©rimentations menÃ©es, en plus de faire varier le batch size et le taux d'augmentation des donnÃ©es, consistaient Ã  faire varier le nombre de cannaux et et le taux de dropout des diffÃ©rentes couches.

Les combinaisons suivantes ont Ã©tÃ© testÃ©es :

* Batch size 32 et 64

* Nombre de cannaux dans les 2 premiÃ¨res couches de classification: 64,128,256,512,1024,2048

* Taux de dropout entre chaque couches: 0%, 10%, 20%

Cette architecture a permis d'obtenir des scores compris entre  70% et 77% de prÃ©cision.

Ces entrainements ont rÃ©vÃ©lÃ© une trÃ¨s forte tendance du modÃ¨le Ã  faire du sur-apprentissage sur notre jeu de donnÃ©es, avec une prÃ©cision sur les donnÃ©es de test rapidement Ã  100% quelque soit l'architecture et les paramÃ¨tres employÃ©s:

![newplot.png](img/newplot.png)

## 3.2. EfficientNetB1

### 3.2.1 Environnement de travail

L'entraÃ®nement du model EfficientNetB1 a Ã©tÃ© rÃ©alisÃ© sur un environnement sans GPU, ce qui pose certaines contraintes en termes de performance (en particulier sur la mÃ©moire vive). Afin d'entraÃ®ner un modÃ¨le efficace malgrÃ© ces limitations, tout en optimisant l'usage des ressources disponibles.  Une astuce fournit par Google est utilisÃ© pour permettre un entrainement sur CPU, [en configurant la fonction expÃ©rimental AutoShardPolicy](https://www.tensorflow.org/datasets/performances).

### 3.2.2 PrÃ©paration Ã  la prÃ©diction

Les labels des images ont Ã©tÃ© rÃ©cupÃ©rÃ©s Ã  partir de `.class_names`, fournissant ainsi une liste ordonnÃ©e des classes.

Une fonction Python personnalisÃ©e `get_champi_name()` a Ã©tÃ© utilisÃ©e pour organiser les noms des labels en fonction des besoins spÃ©cifiques du projet pour la prÃ©diction.

### 3.2.3 Entrainement du modÃ¨le

Les images dâ€™entrÃ©e ont Ã©tÃ© redimensionnÃ©es en 224x224 pixels, conformÃ©ment aux attentes du modÃ¨le prÃ©-entraÃ®nÃ©. Ce format est communÃ©ment utilisÃ© pour des modÃ¨les basÃ©s sur **ImageNet**.

Le modÃ¨le utilisÃ© est prÃ©-entraÃ®nÃ© sur le dataset ImageNet, qui contient plus de 14 millions d'images et 1000 classes. Ce modÃ¨le, comprenant environ 9 millions de paramÃ¨tres, permet dâ€™obtenir une base solide pour la classification des images.

Le `batch_size` est fixÃ© Ã  32, un compromis entre la vitesse dâ€™entraÃ®nement et lâ€™utilisation de la mÃ©moire vive. La structure du modÃ¨le a Ã©tÃ© adaptÃ©e pour intÃ©grer la couche finale de classification avec activation softmax, en fonction du nombre de classes cibles.

### 3.2.4 Ã‰valuation des rÃ©sultats

Sur 23  classes, le modÃ¨le a atteint une prÃ©cision moyenne de 98 % lors de l'entraÃ®nement, indiquant une forte capacitÃ© Ã  gÃ©nÃ©raliser sur les donnÃ©es d'entraÃ®nement.

Les rÃ©sultats sur les donnÃ©es de validation varient entre 80 % et 90 %, en fonction des ajustements apportÃ©s aux hyperparamÃ¨tres. Cela montre que le modÃ¨le a une bonne capacitÃ© de gÃ©nÃ©ralisation, mais pourrait Ãªtre affinÃ© pour Ã©viter le surapprentissage.

Certaines classes de champignion sont visiblement trÃ¨s problÃ¨matique avec ce modÃ¨le et ne parviennent quasiment jamais Ã  effectuer des prÃ©dictions juste.

![efficientnetb1_matrice_01](./img/efficientnetb1_matrice_01.png)

Il est notÃ© que certaines espÃ¨ces de champignons, comme par exemple le Stropharia ambigua *(classe 14)* est souvent prÃ©dite comme une autre espece, la seule nuance qui permette de diffÃ©rencier Ã©tant la couleur jaunÃ¢tre propre a cette espÃ¨ce, nous pouvons en dÃ©duire que ce modÃ¨le n'est pas  trÃ¨s performant sur la prise en compte des  nuances de couleurs.

![champi_diff](./img/champi_diff.png)

*Les Stropharia ambigua sont prÃ©dites sans prendre en compte leur couleur jaunÃ¢tre.*

### 3.2.5 Optimisation

De nouveaux essais sont effectuÃ©es sur 16 classes uniquement pour volontairement exclure les classes problÃ©matiques, avec une augmentation des donnÃ©s et un nombre d'epoch plus gÃ©nÃ©reux.

![dataset](./img/efficientnetb1_dataset.png)

Ajout de callbacks : **ReduceLROnPlateau** pour amÃ©liorer la dÃ©cente de gradient et **EarlyStopping** pour eviter le sur-entrainement.

Deux couches Dropout avec un  taux de 0.5 ont est ajoutÃ©s au rÃ©seau pour le rendre plus robuste.

Les prÃ©cÃ©dents rÃ©sultats montrent que les predictions sont clairement moins fiables sur les derniÃ¨res classes. Ceci est causÃ© car les donnÃ©es ne sont pas mÃ©langÃ©s alÃ©atoirement sur les diffÃ©rents jeux de donnÃ©. Ainsi, un Shuffle est activÃ© pour forcer l'entrainement des donnÃ©es dans un ordre alÃ©atoire.

L'entrainement s'arrÃªte aprÃ¨s seulement 4 epochs grÃ¢ce au EarlyStopping, le sur-entrainement sur ce modÃ¨le interevient trÃ¨s rapidement de par sa nature, mais offre de bonnes perfomances.

![metrics](./img/efficientnetb1_metrics.png)

![matrix_02](./img/efficientnetb1_matrix_02.png)

![predictions](./img/efficientnetb1_predictions.png)

### 3.2.6 Conclusion

L'entraÃ®nement du modÃ¨le EfficientNetB1 sur un environnement sans GPU a permis d'obtenir des rÃ©sultats satisfaisants malgrÃ© les limitations matÃ©rielles. En optimisant l'utilisation des ressources, notamment grÃ¢ce Ã  l'astuce de la configuration `AutoShardPolicy`, le modÃ¨le a pu tirer parti d'un environnement CPU tout en maintenant de bonnes performances.

L'utilisation d'un modÃ¨le prÃ©-entraÃ®nÃ© sur ImageNet fourni une base solide pour la classification. De plus, la gestion personnalisÃ©e des labels a permis une adaptation efficace aux besoins spÃ©cifiques du projet. Nous constatons cependant que ce modÃ¨le n'est malheureusement pas trÃ¨s performant lorsqu'il s'agit de nuancer les couleurs des diffÃ©rentes Ã©spÃ¨ces.

Les performances du modÃ¨le ont montrÃ© une prÃ©cision d'entraÃ®nement remarquable Ã  96% et une prÃ©cision de validation de 86%.

Sur le jeu de test, les scores sont cependant plus interessants :

| Accuracy        | Precision       | Recall          | F1-score        |
| --------------- | --------------- | --------------- | --------------- |
| 0.9286764705882 | 0.9336224871829 | 0.9286764705882 | 0.9290201971718 |

Bien que ces rÃ©sultats soient encourageants, ils rÃ©vÃ¨lent Ã©galement des marges de progression, notamment pour affiner les scores de prÃ©cision sur le jeu d'Ã©valuation.

Ces conclusions ouvrent la voie Ã  des pistes d'amÃ©lioration, telles que l'optimisation des hyperparamÃ¨tres et une meilleure gestion des donnÃ©es pour minimiser le risque de sur-apprentissage, EfficientNetB1 Ã©tant particuliÃ¨rement senssible au sur-entrainement.

Bien que l'entrainement sur CPU est satisfaisant, effectuer ces expÃ©rimentations avec un GPU devrais offrir un gain de vitesse.

## 3.3 ResNet50

AprÃ¨s avoir explorÃ© EfficientNetB1, nous avons dÃ©cidÃ© de tester ResNet50. Cette architecture se distingue par ses blocs rÃ©siduels qui facilitent l'entraÃ®nement de rÃ©seaux trÃ¨s profonds en ajoutant des connexions directes entre les couches. Pour la reconnaissance des champignons, ResNet50 peut Ãªtre particuliÃ¨rement intÃ©ressant en raison de sa capacitÃ© Ã  extraire des caractÃ©ristiques complexes tout en maintenant une efficacitÃ© computationnelle, ce qui est crucial pour des tÃ¢ches de classification fines comme celle-ci.

### 3.3.1. ModÃ¨le de base

Le modÃ¨le est basÃ© sur **ResNet50**, prÃ©-entraÃ®nÃ© sur le jeu de donnÃ©es **ImageNet**. Nous avons enlevÃ© la partie supÃ©rieure du modÃ¨le (le "top") pour adapter le rÃ©seau prÃ©-entraÃ®nÃ© Ã  notre tÃ¢che spÃ©cifique de reconnaissance des champignons. La partie supÃ©rieure d'un modÃ¨le prÃ©-entraÃ®nÃ© est gÃ©nÃ©ralement conÃ§ue pour des classes spÃ©cifiques du jeu de donnÃ©es d'origine, comme ImageNet. En retirant cette partie, nous pouvons ajouter des couches adaptÃ©es Ã  notre propre ensemble de classes, ce qui permet au modÃ¨le de s'ajuster aux spÃ©cificitÃ©s de notre tÃ¢che de classification multiclasse. Nous avons ajoutÃ© une couche de **GlobalAveragePooling2D** suivie d'une couche **Dense** de 1024 neurones (taille couramment utilisÃ©e dans de nombreux rÃ©seaux de neurones pour les couches cachÃ©es) avec activation **ReLU**. La derniÃ¨re couche de sortie est une couche **Dense** avec autant de neurones que de classes dans les donnÃ©es, utilisant une activation **softmax** pour la classification multiclasse.

Les couches du modÃ¨le prÃ©-entraÃ®nÃ© ResNet50 ont Ã©tÃ© gelÃ©es (non-entraÃ®nables) pour conserver les poids appris prÃ©cÃ©demment et Ã©viter de modifier ces paramÃ¨tres durant l'entraÃ®nement. Le modÃ¨le a Ã©tÃ© compilÃ© avec l'optimiseur **Adam** et une faible valeur d'apprentissage (learning rate = 1e-4). La perte utilisÃ©e est **categorical crossentropy**, avec une mÃ©trique dâ€™Ã©valuation sur la **prÃ©cision**.

![resnet50_model1.png](img/resnet50_model1.png)

**RÃ©sultats** obtenus :

PrÃ©cision d'entraÃ®nement : Le modÃ¨le montre une prÃ©cision qui commence Ã  71 % et atteint presque 100 % (99,96 %) Ã  la fin de lâ€™entraÃ®nement. Cela montre que le modÃ¨le apprend trÃ¨s bien les donnÃ©es dâ€™entraÃ®nement, mais cela suggÃ¨re aussi un risque de **surapprentissage** (overfitting).

PrÃ©cision de validation : La prÃ©cision de validation commence relativement Ã©levÃ©e Ã  81 %, mais fluctue au fil des Ã©poques, se stabilisant autour de 84 %. Le modÃ¨le gÃ©nÃ©ralise relativement bien, mais ne montre pas d'amÃ©lioration significative aprÃ¨s quelques itÃ©rations, suggÃ©rant un plateau dans l'apprentissage.

Perte de validation : La perte de validation diminue lÃ©gÃ¨rement au dÃ©but, mais Ã  partir de la cinquiÃ¨me Ã©poque, elle commence Ã  augmenter. Cela reflÃ¨te encore une fois un surapprentissage, car la perte dâ€™entraÃ®nement continue de baisser tandis que la perte de validation augmente. Cela signifie que le modÃ¨le se spÃ©cialise trop sur les donnÃ©es dâ€™entraÃ®nement et ne parvient pas Ã  bien gÃ©nÃ©raliser sur de nouvelles donnÃ©es.

### 3.3.2. ModÃ¨les ajustÃ©s

1) **Ajout de Dropout (0.5)**
   Le Dropout a Ã©tÃ© ajoutÃ© aprÃ¨s la couche de GlobalAveragePooling2D et aprÃ¨s la couche Dense, avec un taux de 0,5. Cela permet de rÃ©duire le surapprentissage (overfitting) en dÃ©sactivant alÃ©atoirement 50 % des neurones pendant l'entraÃ®nement. Cela rend le modÃ¨le moins dÃ©pendant de certains neurones spÃ©cifiques et amÃ©liore sa capacitÃ© de gÃ©nÃ©ralisation.
   
   **RÃ©gularisation L2 (0.001)**
   Une rÃ©gularisation L2 a Ã©tÃ© appliquÃ©e sur la couche Dense. Cette technique pÃ©nalise les poids excessivement Ã©levÃ©s, contribuant Ã  rÃ©duire le surapprentissage en encourageant des poids plus petits. Cela aide Ã  crÃ©er un modÃ¨le plus stable et capable de mieux gÃ©nÃ©raliser aux nouvelles donnÃ©es.
   
   **RÃ©sultats** : 
   La prÃ©cision d'entraÃ®nement atteint 77 %, tandis que la prÃ©cision de validation passe de 70 % Ã  80 % avec une perte de validation en baisse constante, montrant que la rÃ©gularisation par Dropout et L2 aide Ã  mieux gÃ©nÃ©raliser et Ã  rÃ©duire le surapprentissage.

2) **Unfreezed layers**
   
   Les 10 derniÃ¨res couches du modÃ¨le de base ResNet50 ont Ã©tÃ© "dÃ©figÃ©es" pour Ãªtre entraÃ®nables, ce qui permet Ã  ces couches d'affiner leurs poids pendant l'entraÃ®nement. L'apprentissage est effectuÃ© avec un taux d'apprentissage plus bas (1e-5) pour Ã©viter une mise Ã  jour trop rapide des poids, et ce sur 10 Ã©poques.
   
   **RÃ©sultats** : Le surapprentissage est probable, car l'exactitude en entraÃ®nement est trÃ¨s Ã©levÃ©e, mais l'exactitude en validation stagne et la perte en validation ne diminue pas significativement.

3) **RÃ©gularisation et Dropout** :  Deux couches de Dropout Ã  50% et la rÃ©gularisation L2 sont ajoutÃ©es pour limiter le surapprentissage, en rÃ©duisant la dÃ©pendance du modÃ¨le Ã  certaines connexions spÃ©cifiques.
   
   **Optimisation de l'entraÃ®nement**
   **Early Stopping** : Le modÃ¨le arrÃªte l'entraÃ®nement si la perte en validation ne s'amÃ©liore plus aprÃ¨s 3 Ã©poques, tout en restaurant les meilleurs poids, Ã©vitant un surapprentissage inutile.
   
   **RÃ©duction du taux d'apprentissage (ReduceLROnPlateau)** : Ce callback diminue progressivement le taux d'apprentissage si la validation stagne, permettant des ajustements plus fins dans les derniÃ¨res Ã©tapes d'entraÃ®nement.
   
   **Augmentation des epochs** : 
   Le nombre d'Ã©poques est passÃ© Ã  50, en combinaison avec les mÃ©canismes de contrÃ´le du surapprentissage, pour permettre au modÃ¨le d'explorer un espace plus large de solutions tout en conservant une bonne gÃ©nÃ©ralisation.
   
   **RÃ©sultats** : L'exactitude d'entraÃ®nement dÃ©passe 94%, mais l'exactitude de validation progresse lentement et se stabilise, indiquant un possible surapprentissage. La perte d'entraÃ®nement diminue, tandis que la perte de validation augmente aprÃ¨s quelques Ã©poques, renforÃ§ant l'hypothÃ¨se de surapprentissage. Le taux d'apprentissage, initialement Ã  1e-5, a Ã©tÃ© rÃ©duit Ã  2e-6 aprÃ¨s la 7e Ã©poque, montrant que le modÃ¨le a atteint un plateau tÃ´t.

4) **Augmentation du taux de dropout** Ã  0,7 pour rÃ©duire la dÃ©pendance excessive aux neurones spÃ©cifiques et prÃ©venir le surapprentissage. 5 couches de ResNet50 sont dÃ©sormais dÃ©gelÃ©es pour affiner davantage l'apprentissage. 
   Le **callback EarlyStopping** a une patience augmentÃ©e Ã  5 pour permettre au modÃ¨le de continuer l'entraÃ®nement plus longtemps avant d'arrÃªter si la validation ne s'amÃ©liore pas, et le **ReduceLROnPlateau** rÃ©duit le taux d'apprentissage plus progressivement, avec un plancher fixÃ© Ã  1e-6.
   Augmentation de la **rÃ©gularisation L2** dans la couche de sortie Ã  0.01 pour mieux contrÃ´ler le surapprentissage.

![resnet50_model_last.png](img/resnet50_model_last.png)

**RÃ©sultats** L'exactitude d'entraÃ®nement atteint 95%, mais la prÃ©cision de validation stagne autour de 80%, suggÃ©rant un surapprentissage. La perte de validation diminue au dÃ©but mais augmente ensuite, ce qui confirme Ã©galement un surapprentissage. Le taux d'apprentissage rÃ©duit considÃ©rablement aprÃ¨s la 12e Ã©poque, montrant que le modÃ¨le a atteint un plateau prÃ©coce dans l'entraÃ®nement.

### Conclusion

MalgrÃ© une haute prÃ©cision d'entraÃ®nement (95%), le modÃ¨le montre encore du **surapprentissage**, avec une prÃ©cision de validation stagnante autour de 80% et une perte de validation croissante.

Causes possibles :

- Le modÃ¨le prÃ©-entraÃ®nÃ© sur ImageNet peut ne pas capturer suffisamment les caractÃ©ristiques spÃ©cifiques des champignons.
- Les donnÃ©es d'entraÃ®nement pourraient Ãªtre insuffisantes pour une gÃ©nÃ©ralisation efficace.
- Le taux d'apprentissage pourrait ne pas Ãªtre optimal pour ce problÃ¨me spÃ©cifique.

Prochaines Ã©tapes :

- Augmenter la taille et la diversitÃ© des donnÃ©es d'entraÃ®nement sur les champignons.
- Ajuster ou simplifier l'architecture du modÃ¨le.
- Ajouter des techniques de rÃ©gularisation supplÃ©mentaires.
- ExpÃ©rimenter avec des modÃ¨les spÃ©cifiquement entraÃ®nÃ©s sur des donnÃ©es de champignons ou utiliser des techniques de transfert learning adaptÃ©es.

## 3.4. ResNet18

ResNet18 est comme ResNet50 un rÃ©seau de neurones convolutifs utilisant des connexions rÃ©siduelles, mais avec une profondeur de 18 couches seulement.

Les essais menÃ©s avec ResNet18 ont Ã©tÃ© immÃ©diatement concluants avec un score de prÃ©cision d'environs 97% sur notre dataset (23 classe avec en moyenne 166 images par classe).

![resnet18_01](./img/resnet18_01.png)

DiffÃ©rents essais on permis d'estimer le nombre d'images necessaires pour obtenir un niveau de prÃ©cision satsfaisant. Ainsi on constate qu'avec un dataset comprenant seulement 80 images par classe on atteint une prÃ©cision de 97%. Avec seulement 30 images par classe on reste au dessus de 90% de prÃ©cision, et mÃªme avec seulement 10 images par classe on reste au dessus de 80% de prÃ©cision.

![resnet18_02](./img/resnet18_02.png)

## 3.5 JarviSpore

Suite aux rÃ©sultats mitigÃ©s sur le transfert Learning, nous avons pris l'initiative de crÃ©er un modÃ¨le de zÃ©ro avec comme support les cours DataScientest et des livres.

Nous avons investi dans un PC avec carte graphique RTX 3090 disposant de 24 Go de VRAM. Notre PC dispose de 192 Go de RAM. Processeur i9 14900k.

Nous avons ensuite pris le parti de tester dans un environnement WSL2. Cela nous permettait d'utiliser les derniÃ¨res versions de TensorFlow, Cuda, Cudnn et Keras.

AprÃ¨s l'installation, nous avons construit le modÃ¨le dans VSCode, mais lors des entraÃ®nements, des problÃ¨mes de mÃ©moire nous ont compliquÃ©s la tÃ¢che.

Nous avons dÃ©ployÃ© un environnement sous Windows en utilisant d'anciennes versions de TensorFlow, Cuda â€¦

Pour assurer la compatibilitÃ© des bibliothÃ¨ques utilisÃ©es et de leurs versions, car la compatibilitÃ© TensorFlow sous Windows s'arrÃªte Ã  la version 2.10 :

```
numpy : 1.26.4
tensorflow : 2.10.0
matplotlib : 3.9.2
scikit-learn : 1.5.2
PIL : 10.4.0
cv2 : 4.10.0
pandas : 2.2.3
```

Ce modÃ¨le effectue l'entraÃ®nement, l'Ã©valuation et l'interprÃ©tation d'un modÃ¨le de rÃ©seau de neurones convolutif (CNN) pour une tÃ¢che de classification d'images. Voici les diffÃ©rentes Ã©tapes et le processus utilisÃ©s :

1. Importation des BibliothÃ¨ques  
   Nous commenÃ§ons par importer les bibliothÃ¨ques nÃ©cessaires pour la manipulation des donnÃ©es, l'entraÃ®nement du modÃ¨le, l'Ã©valuation et la visualisation des rÃ©sultats. Les bibliothÃ¨ques incluent TensorFlow pour la construction du modÃ¨le, NumPy pour les calculs numÃ©riques, Pandas pour la gestion des donnÃ©es et OpenCV pour le traitement des images.

2. Extraction des Versions des BibliothÃ¨ques  
   Nous vÃ©rifions les versions des bibliothÃ¨ques utilisÃ©es afin d'assurer la compatibilitÃ© des versions.

3. Chargement des Datasets (structurÃ©es et non structurÃ©es)  
   Nous dÃ©finissons les chemins pour les datasets d'entraÃ®nement, de validation et de test. Nous utilisons la fonction image_dataset_from_directory pour charger les images en les redimensionnant Ã  la taille (224, 224) avec un batch size de 32 images. Les ensembles de donnÃ©es sont ensuite configurÃ©s pour Ãªtre mis en cache en mÃ©moire vive, prÃ©chargÃ©s et optimisÃ©s.

4. Chargement des Classes  
   Nous chargeons les noms des classes Ã  partir d'un fichier CSV (API MushroomObserver) pour obtenir la liste des classes disponibles. Cela permet au modÃ¨le d'associer les indices des classes avec les noms rÃ©els lors de l'affichage des rÃ©sultats.

5. Construction du ModÃ¨le Convolutionnel  
   Nous construisons un CNN personnalisÃ© avec plusieurs couches de convolution suivies de la normalisation par lots (Batch Normalization), du sous-Ã©chantillonnage (MaxPooling) et d'une couche de sortie utilisant softmax pour la classification des 23 classes. Les couches de convolution permettent d'extraire les caractÃ©ristiques des images, tandis que les couches denses Ã  la fin effectuent la classification.

6. Compilation du ModÃ¨le  
   Le modÃ¨le est compilÃ© avec l'optimiseur Adam et la fonction de perte sparse_categorical_crossentropy, adaptÃ©e Ã  la classification multi-classes avec des Ã©tiquettes sous forme d'entiers.

7. Ajout de l'Early Stopping et du Model Checkpoint  
   Nous configurons des callbacks pour arrÃªter l'entraÃ®nement si la prÃ©cision de validation n'augmente plus aprÃ¨s 5 Ã©poques (early stopping) et pour sauvegarder le meilleur modÃ¨le lors de l'entraÃ®nement (ModelCheckpoint).

8. Gestion du DÃ©sÃ©quilibre des Classes  
   Nous vÃ©rifions le dÃ©sÃ©quilibre des classes dans l'ensemble d'entraÃ®nement. Si certaines classes sont moins reprÃ©sentÃ©es, nous utilisons des pondÃ©rations de classe (class_weight) pour accorder plus d'importance aux classes sous-reprÃ©sentÃ©es afin d'amÃ©liorer la gÃ©nÃ©ralisation du modÃ¨le.

9. EntraÃ®nement du ModÃ¨le  
   Le modÃ¨le est entraÃ®nÃ© sur 20 Ã©poques, en utilisant les pondÃ©rations de classe pour mieux gÃ©rer les dÃ©sÃ©quilibres. Les callbacks configurÃ©s permettent de surveiller la performance et de sauvegarder le meilleur modÃ¨le.

10. GÃ©nÃ©ration de la Matrice de Confusion  
    AprÃ¨s l'entraÃ®nement, nous gÃ©nÃ©rons une matrice de confusion sur l'ensemble de validation pour Ã©valuer la capacitÃ© du modÃ¨le Ã  classifier correctement les images. La matrice de confusion est affichÃ©e avec les noms des classes pour faciliter l'interprÃ©tation des rÃ©sultats.

11. Visualisation des Courbes d'EntraÃ®nement  
    Nous affichons les courbes de prÃ©cision et de perte pour les ensembles d'entraÃ®nement et de validation, ce qui nous permet de visualiser l'Ã©volution des performances du modÃ¨le pendant l'entraÃ®nement.

12. Sauvegarde du ModÃ¨le et MÃ©tadonnÃ©es  
    Nous sauvegardons le modÃ¨le entraÃ®nÃ© au format .keras ainsi que les mÃ©tadonnÃ©es (date d'entraÃ®nement, prÃ©cision sur l'ensemble de test, nombre d'Ã©poques). Cela permet de documenter le modÃ¨le pour un suivi ultÃ©rieur.

13. Test et Ã‰valuation du ModÃ¨le sur l'Ensemble de Test  
    Nous testons le modÃ¨le sur le jeu de donnÃ©es de test pour obtenir la prÃ©cision finale et Ã©valuer sa performance gÃ©nÃ©rale.

14. Affichage Grad-CAM  
    Nous implÃ©mentons Grad-CAM pour visualiser les activations des couches de convolution du modÃ¨le. Cette technique permet d'afficher les rÃ©gions de l'image qui ont le plus contribuÃ© Ã  la dÃ©cision du modÃ¨le. Les rÃ©sultats sont affichÃ©s pour cinq images alÃ©atoires du jeu de test.

RÃ©sultats Attendues  

- PrÃ©cision du ModÃ¨le : La mÃ©trique mesurÃ©e est la prÃ©cision, elle permet de mesurer le pourcentage de classifications correctes effectuÃ©es.  
- InterprÃ©tabilitÃ© avec Grad-CAM : Les heatmaps gÃ©nÃ©rÃ©es par Grad-CAM doivent indiquer les parties pertinentes de l'image, ce qui aide Ã  comprendre le fonctionnement du modÃ¨le.  
- GÃ©nÃ©ralisation : Avec l'utilisation des callbacks et des pondÃ©rations de classe, le modÃ¨le doit Ã©viter le sur-apprentissage et bien gÃ©nÃ©raliser sur les donnÃ©es de validation et de test.

Ces Ã©tapes permettent de construire un modÃ¨le performant pour la classification d'images, tout en prenant en compte les dÃ©sÃ©quilibres de classe et en offrant des outils d'interprÃ©tation des rÃ©sultats.

Lien vers le modÃ¨le sur Hugging Face : https://huggingface.co/YvanRLD/JarviSpore

![jarvispore_001](./img/jarvispore_001.png)
![jarvispore_002](./img/jarvispore_002.png)
![jarvispore_002](./img/jarvispore_003.png)

# 4. InterprÃ©tation des rÃ©sultats avec Grad-CAM

Pour mieux comprendre les rÃ©sultats et les dÃ©cisions prises par les algorithmes, nous avons utilisÃ© **Grad-CAM** (Gradient-weighted Class Activation Mapping), une technique puissante d'interprÃ©tation des modÃ¨les de deep learning, en particulier pour la classification d'images. Cette mÃ©thode permet de visualiser les rÃ©gions d'une image qui influencent le plus les dÃ©cisions d'un modÃ¨le. En gÃ©nÃ©rant des cartes thermiques (heatmaps) superposÃ©es sur les images d'entrÃ©e, Grad-CAM met en Ã©vidence les caractÃ©ristiques jugÃ©es essentielles par le modÃ¨le pour ses prÃ©dictions.

Pour crÃ©er ces cartes thermiques, on commence par calculer les gradients associÃ©s Ã  la classe prÃ©dite, en les reliant aux cartes de caractÃ©ristiques issues de la derniÃ¨re couche de convolution. Ces gradients sont ensuite moyennÃ©s pour obtenir une vue d'ensemble, qui sert Ã  ajuster les cartes de caractÃ©ristiques, mettant ainsi en lumiÃ¨re les zones les plus importantes pour la classification.

Avec Grad-CAM, nous pouvons mieux comprendre les performances de nos modÃ¨les en analysant visuellement leurs points d'attention. Cette approche nous aide Ã  identifier les forces et les faiblesses des modÃ¨les, Ã  dÃ©celer d'Ã©ventuels biais et Ã  approfondir notre comprÃ©hension des dÃ©cisions prises par les algorithmes.

Le graphique ci-dessous illustre des exemples de cartes thermiques obtenues via EfficientNetB1 et ResNet50 pour quatre classes diffÃ©rentes de champignons.

 ![gradcam.png](./img/gradcam.png)

Les "zones chaudes" (zones rouges et jaunes des cartes thermiques) indiquent les rÃ©gions sur lesquelles le modÃ¨le se concentre. En gÃ©nÃ©ral, ces zones chaudes correspondent Ã  certaines parties du champignon, mais la zone de concentration varie selon la classe de champignon (par exemple, la tige par rapport Ã  la tÃªte du champignon, le bord de la tÃªte, etc.). Il est intÃ©ressant de noter que, pour l'image contenant deux champignons, ResNet50 performe mieux en identifiant les deux, tandis qu'EfficientNet se concentre principalement sur un seul champignon.
Cependant, pour la photo avec la prÃ©sence de la main, ResNet50 Ã©tait complÃ¨tement perdu et ne se concentrait pas du tout sur le champignon, tandis qu'EfficientNet l'identifiait mieux. 

En somme, ces rÃ©sultats soulignent l'importance d'une analyse approfondie pour mieux comprendre les performances de chaque modÃ¨le dans des situations variÃ©es.

# 5. Conclusion

## 5.1 Comparaison des resultats

Les diffÃ©rents essais rÃ©alisÃ©s ont mis en Ã©vidence d'importantes diffÃ©rences de rÃ©sultats obtenus avec divers modÃ¨les sur un mÃªme jeu de donnÃ©es. Alors que certains modÃ¨les, comme VGG16, affichent des limites significatives pour ce cas d'utilisation, d'autres, tels que ResNet18, ont dÃ©montrÃ© d'excellentes performances.

En poursuivant notre analyse, nous avons Ã©galement comparÃ© ResNet18 et ResNet50. Cette comparaison montre qu'un modÃ¨le plus profond n'est pas toujours synonyme de meilleures performances ; au contraire, sur un petit jeu de donnÃ©es, un modÃ¨le plus complexe peut s'avÃ©rer moins performant.

Dans le cadre de notre projet, nous avons intÃ©grÃ© l'approche MLflow pour amÃ©liorer le suivi et la gestion de nos expÃ©riences en apprentissage automatique.

Ce dernier est utilisÃ© pour tracer chaque Ã©tape du processus expÃ©rimental, notamment les paramÃ¨tres, les mÃ©triques, et les artefacts des modÃ¨les.

Nous avons configurÃ© un serveur de tracking, dÃ©fini un projet spÃ©cifique pour organiser nos expÃ©rimentions.

Cette intÃ©gration permet de centraliser et comparer les mÃ©triques et de faciliter le dÃ©ploiement ultÃ©rieur des modÃ¨les retenus.

Ainsi, nous pouvons suivre de maniÃ¨re systÃ©matique et efficace les progrÃ¨s rÃ©alisÃ©s dans notre projet. Les captures suivantes rÃ©sument donc les rÃ©sultats de l'ensemble du projet :

![mlflow](./img/mlflow.png)
![mlflow2](./img/mlflow2.png)

## 5.2 InterpretabilitÃ©

Les diffÃ©rentes Ã©valuation des modÃ¨les effectuÃ©s sur des donnÃ©es de tests montrent que de faÃ§on global, les modÃ¨les ont tous tendance Ã  effectuer de la sur-interprÃ©tation et gÃ¨rent particuliÃ¨rement mal les couleurs des champignons. 

En effet les mÃ©thodes Grad-Cam permettent de visualiser cette tendance Ã  prendre en compte des zones prÃ©cises, sans se concentrer sur les zones trÃ¨s colorÃ©s. La couleur est pourtant l'un des points les plus importants, les modÃ¨les montrent tous de grandes faiblesse pour diffÃ©rencier deux champignons physiquement identique avec la couleur comme seul Ã©lÃ©ment de diffÃ©renciation ou encore de simplement localiser un champigon, mÃªme de couleur vive, si le fond de la photo contient des Ã©lÃ©ments avec une forte luminositÃ© proche du blanc.

![jarvispore_004](./img/jarvispore_004.png)

## 5.3 Technique

Nous pouvons noter que si les modÃ¨les avec une architeture les plus basiques (Lenet) offre des resultats trÃ¨s moyen, ceux en transfert learning offrent cependant des resultats performants mÃªme si rapidement sujet Ã  un sur-apprentissage malgrÃ¨s nos essais avec diffÃ©rentes technique d'optimisation.

Nous concluons sur le fait que la conception d'un modÃ¨le sur-mesure, avec une architecture complexe, bien que trÃ¨s fastidieux, permet d'obtenir des mÃ©triques et rapports de classification plus performant Ã  tout les niveaux.

En effet, dÃ©cision est prise d'implÃ©menter un modÃ¨le nommÃ© JarviSpore, solution modulable au fur et Ã  mesure de l'avancÃ©e de nos connaissances. Celui-ci est arrivÃ© Ã  maturitÃ© et prÃ©sente des performances supÃ©rieures.

# 6. Pour aller plus loin

## ModÃ¨les Open-Source

L'utilisation de modÃ¨les plus rÃ©cents accessible sur HuggingFace nous permettrais trÃ¨s surement  d'obtenir encore de meilleurs performance de prÃ©cision.

Nous expÃ©rimentons l'utilisation d'un modÃ¨le communautaire de classification prÃ©-entrainÃ© sur 100 espÃ¨ces de champignons russe.

Ce modÃ¨le, partagÃ© par Dmytro Iakubovskyi, est entrainÃ© sur 233480 images et se base sur l'architecture ViT (85.9M de paramÃ¨tres).

[ğŸ¤— Mid-ViT par dima806 - HuggingFace](https://huggingface.co/dima806/mushrooms_image_detection)

## RÃ©seaux Kolmogorov-Arnold

Les MLP (rÃ©seaux de neurones multicouches), bien qu'utilisÃ©s dans de nombreux contextes, sont souvent sujets Ã  l'overfitting en raison de leur grande flexibilitÃ©, et comportent de nombreux paramÃ¨tres difficiles Ã  interprÃ©ter, ce qui limite leur utilitÃ© dans certaines applications. 

RÃ©cemment, les rÃ©seaux Kolmogorov-Arnold (KAN) ont Ã©tÃ© proposÃ©s comme une alternative prometteuse (article : https://arxiv.org/abs/2404.19756, GitHub : https://github.com/KindXiaoming/pykan). 

Contrairement aux MLP, qui utilisent des fonctions non linÃ©aires fixes comme ReLU ou Tanh, les KAN exploitent des B-splines, des polynÃ´mes par morceaux, pour modÃ©liser les donnÃ©es de maniÃ¨re plus souple et ajustÃ©e. Cela permet d'amÃ©liorer l'interprÃ©tabilitÃ© des modÃ¨les et de rÃ©duire le nombre de paramÃ¨tres, rendant les KAN plus efficaces et potentiellement moins sensibles Ã  l'overfitting.

Cependant, bien que les KAN prÃ©sentent de nombreux avantages thÃ©oriques, ils restent instables, avec des rÃ©sultats sensibles aux hyperparamÃ¨tres choisis, ce qui nÃ©cessite des ajustements soigneux pour chaque tÃ¢che. 

Pour les prochains tests, il sera crucial d'explorer davantage cette nouvelle architecture, de tester son potentiel de gÃ©nÃ©ralisation sur des donnÃ©es variÃ©es, et d'Ã©valuer dans quelle mesure elle peut remplacer les MLP dans des architectures complexes.
